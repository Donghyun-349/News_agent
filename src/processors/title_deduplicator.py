#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 1.5: Title-Based Deduplication Processor (Refactored)

ì œëª©ì´ ë™ì¼í•œ ê¸°ì‚¬ë“¤ì„ ë³‘í•©í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
Lane í• ë‹¹ ë¡œì§ì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import logging
import re
import html
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from datetime import datetime

from config.source_hierarchy import get_source_tier

logger = logging.getLogger(__name__)


class TitleDeduplicator:
    """ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±° í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, db_adapter):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_adapter: DatabaseAdapter instance
        """
        self.db_adapter = db_adapter
    
    @staticmethod
    def normalize_title(title: str) -> str:
        """
        ì œëª© ì •ê·œí™”: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        
        Args:
            title: ì›ë³¸ ì œëª©
            
        Returns:
            ì •ê·œí™”ëœ ì œëª©
        """
        if not title:
            return ""
        
        # ê³µë°± ì •ê·œí™”
        normalized = re.sub(r'\s+', ' ', title)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë”°ì˜´í‘œ, ëŒ€ê´„í˜¸ ë“±)
        normalized = normalized.strip()
        # ì†Œë¬¸ì ë³€í™˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        normalized = normalized.lower()
        
        return normalized
    
    def select_representative(self, articles: List[Dict[str, Any]]) -> Tuple[Dict[str, Any], int, List[int], List[str]]:
        """
        ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ê¸°ì‚¬ ì„ íƒ
        
        Args:
            articles: ë™ì¼ ì œëª©ì˜ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (representative_article, weight, merged_ids, source_list)
        """
        if not articles:
            return None, 0, [], []
        
        # Source tier ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ tier = ë†’ì€ ìš°ì„ ìˆœìœ„)
        sorted_articles = sorted(
            articles,
            key=lambda a: (
                get_source_tier(a.get("source", "")),
                -len(a.get("title", ""))  # ê°™ì€ tierë©´ ì œëª©ì´ ê¸´ ê²ƒ ìš°ì„ 
            )
        )
        
        representative = sorted_articles[0]
        weight = len(articles)
        merged_ids = [a.get("id") for a in articles if a.get("id") is not None]
        source_list = list(set([a.get("source", "") for a in articles]))
        
        return representative, weight, merged_ids, source_list
    
    def deduplicate_by_title(self) -> Dict[str, Any]:
        """
        raw_news í…Œì´ë¸”ì—ì„œ ì œëª©ì´ ë™ì¼í•œ ê¸°ì‚¬ë“¤ì„ ë³‘í•©í•˜ì—¬
        processed_news í…Œì´ë¸”ì— ì €ì¥ (Incremental Update)
        
        ê¸°ì¡´ processed_news ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³ ,
        ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬ ê·¸ë£¹ì€ ê±´ë„ˆë›°ê³  ìƒˆë¡œìš´ ê·¸ë£¹ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        logger.info("\n" + "="*80)
        logger.info("ğŸ”„ Phase 1.5: Title-Based Deduplication (Incremental) ì‹œì‘")
        logger.info("="*80)

        # 0. Clean HTML Entities in DB first
        self.clean_and_update_db_titles()
        
        cursor = self.db_adapter.connection.cursor()
        
        # 0. ì´ë¯¸ ì²˜ë¦¬ëœ raw_id ëª©ë¡ ë¡œë“œ (Set for O(1) lookup)
        cursor.execute("SELECT ref_raw_id FROM processed_news WHERE ref_raw_id IS NOT NULL")
        existing_raw_ids = set(row[0] for row in cursor.fetchall())
        logger.info(f"ğŸ’¾ Found {len(existing_raw_ids)} already processed articles in DB.")
        
        # 1. raw_newsì—ì„œ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        cursor.execute("""
            SELECT id, source, title, snippet, url, published, collected_at, publisher
            FROM raw_news
            WHERE title IS NOT NULL AND title != ''
        """)
        
        raw_articles = []
        for row in cursor.fetchall():
            raw_articles.append({
                "id": row[0],
                "source": row[7] or row[1], # publisherê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ source (category) ì‚¬ìš©
                "title": row[2],
                "snippet": row[3],
                "link": row[4],  # urlì„ linkë¡œ ë§¤í•‘
                "published_date": row[5],  # publishedë¥¼ published_dateë¡œ ë§¤í•‘
                "collected_date": row[6]  # collected_atë¥¼ collected_dateë¡œ ë§¤í•‘
            })
        
        logger.info(f"ğŸ“¥ Loaded {len(raw_articles)} articles from raw_news")
        
        # 2. ì œëª©ìœ¼ë¡œ ê·¸ë£¹í™”
        title_groups = defaultdict(list)
        for article in raw_articles:
            normalized_title = self.normalize_title(article["title"])
            if normalized_title:
                title_groups[normalized_title].append(article)
        
        logger.info(f"ğŸ“Š Grouped into {len(title_groups)} unique titles")
        
        # 3. ê° ê·¸ë£¹ ì²˜ë¦¬ (Incremental Logic)
        total_new_saved = 0
        total_skipped_existing = 0
        total_duplicates_removed = 0
        duplicate_examples = []
        removed_articles = []
        
        for normalized_title, articles in title_groups.items():
            # Check if ANY article in this group has already been processed
            # (If one is processed, we assume the group is handled or represents the same story)
            
            # ê·¸ë£¹ ë‚´ì˜ id ì§‘í•©
            group_ids = set(a["id"] for a in articles)
            
            # êµì§‘í•©ì´ ìˆìœ¼ë©´ (í•˜ë‚˜ë¼ë„ ì²˜ë¦¬ëœ ì ì´ ìˆìœ¼ë©´) -> Skip
            if not group_ids.isdisjoint(existing_raw_ids):
                total_skipped_existing += 1
                continue
            
            # ì—†ìœ¼ë©´ -> New Group -> Process & Insert
            representative, weight, merged_ids, source_list = self.select_representative(articles)
            
            if not representative:
                continue
            
            # DBì— ì €ì¥
            cursor.execute("""
                INSERT INTO processed_news (
                    ref_raw_id, published_at, source_normalized
                ) VALUES (?, ?, ?)
            """, (
                representative["id"],
                representative.get("published_date") or datetime.now(),
                representative["source"]
            ))
            
            total_new_saved += 1
            
            # ì¤‘ë³µ ì œê±° í†µê³„ (ì‹ ê·œ ì²˜ë¦¬ëœ ê²ƒ ë‚´ì—ì„œë§Œ)
            if weight > 1:
                total_duplicates_removed += (weight - 1)
                
                # ì‚­ì œëœ ê¸°ì‚¬ ì¶”ì  (ëŒ€í‘œ ê¸°ì‚¬ ì œì™¸)
                for article in articles:
                    if article["id"] != representative["id"]:
                        removed_articles.append({
                            "removed_id": article["id"],
                            "removed_source": article["source"],
                            "title": article["title"],
                            "kept_id": representative["id"],
                            "kept_source": representative["source"]
                        })
                
                # ì˜ˆì‹œìš©
                if len(duplicate_examples) < 10:
                    duplicate_examples.append({
                        "title": representative["title"],
                        "weight": weight,
                        "sources": source_list,
                        "selected_source": representative.get("source")
                    })
        
        self.db_adapter.connection.commit()
        
        # 5. ê²°ê³¼ í†µê³„
        stats = {
            "total_raw_articles": len(raw_articles),
            "total_processed_articles": len(existing_raw_ids) + total_new_saved, # Total valid in DB
            "total_new_saved": total_new_saved,
            "total_skipped_existing": total_skipped_existing,
            "total_duplicates_removed": total_duplicates_removed,
            "duplicate_examples": duplicate_examples,
            "removed_articles": removed_articles
        }
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š Deduplication (Incremental) ì™„ë£Œ")
        logger.info("="*80)
        logger.info(f"  ì›ë³¸ ê¸°ì‚¬ Total: {stats['total_raw_articles']}")
        logger.info(f"  ê¸°ì¡´ ì²˜ë¦¬ë¨ (Skip): {stats['total_skipped_existing']} groups")
        logger.info(f"  ì‹ ê·œ ì¶”ê°€ë¨ (New): {stats['total_new_saved']}")
        logger.info(f"  ì‹ ê·œ ì¤‘ ì¤‘ë³µ ì œê±°: {stats['total_duplicates_removed']}")
        
        return stats
    
    def clean_and_update_db_titles(self):
        """raw_newsì˜ íƒ€ì´í‹€ì—ì„œ HTML Entity ë¬¸ì œ í•´ê²° ë° DB ì—…ë°ì´íŠ¸"""
        try:
            cursor = self.db_adapter.connection.cursor()
            
            # Fetch candidates (titles with '&')
            if hasattr(self.db_adapter, 'db_type') and self.db_adapter.db_type == 'sqlite':
                cursor.execute("SELECT id, title FROM raw_news WHERE title LIKE '%&%'")
            else:
                cursor.execute("SELECT id, title FROM raw_news WHERE title LIKE '%&%'")
                
            rows = cursor.fetchall()
            
            updates = []
            for row in rows:
                original_title = row[1]
                if not original_title: continue
                
                # HTML unescape only (No prefix removal here)
                cleaned_title = html.unescape(original_title).strip()
                
                if original_title != cleaned_title:
                    updates.append((cleaned_title, row[0]))
                    
            if updates:
                logger.info(f"ğŸ§¹ Cleaning HTML entities for {len(updates)} articles in raw_news...")
                
                if hasattr(self.db_adapter, 'db_type') and self.db_adapter.db_type == 'sqlite':
                    query = "UPDATE raw_news SET title = ? WHERE id = ?"
                else:
                    query = "UPDATE raw_news SET title = %s WHERE id = %s"
                    
                cursor.executemany(query, updates)
                self.db_adapter.connection.commit()
                logger.info("âœ… Title HTML entity cleaning complete.")
            else:
                logger.info("âœ¨ No HTML entities found in titles to clean.")
        except Exception as e:
            logger.error(f"âš ï¸ Title cleaning failed: {e}")
    
    def process(self) -> Dict[str, Any]:
        """Phase 1.5 ì „ì²´ ì²˜ë¦¬"""
        return self.deduplicate_by_title()
