#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 4: LLM Classification (run_p4.py)

Í∏∞Îä•:
1. processed_news ÌÖåÏù¥Î∏îÏóêÏÑú Í∏∞ÏÇ¨ Ï°∞Ìöå
2. LLM(GPT-4o)ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î∞∞Ïπò Îã®ÏúÑÎ°ú 9Í∞ÄÏßÄ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Î∂ÑÎ•ò
3. Î∂ÑÎ•ò Í≤∞Í≥º (Decision, Category, Reason)Î•º DBÏóê Ï†ÄÏû•
4. Í≤∞Í≥º ÌÜµÍ≥ÑÎ•º Google SheetÏóê Ï∂úÎ†• (4.llm_classification)

Usage:
    python run_p4.py
"""

import sys
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∂îÍ∞Ä
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from config.settings import (
    DB_TYPE, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,
    GOOGLE_SHEET_ID, LOG_LEVEL, OPENAI_API_KEY
)
from config.prompts.classification_pt import get_p4_topic_classification_prompt

# OpenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ (Phase 2ÏôÄ ÎèôÏùºÌïòÍ≤å Ï≤¥ÌÅ¨)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Î°úÍ±∞ ÏÑ§Ï†ï
logger = setup_logger(log_level=LOG_LEVEL)

def get_articles_to_process(db: DatabaseAdapter, limit: int = None, force_all: bool = False) -> List[Dict[str, Any]]:
    """Ï≤òÎ¶¨Ìï† Í∏∞ÏÇ¨ Ï°∞Ìöå"""
    try:
        cursor = db.connection.cursor()
        
        # 1. processed_newsÏóêÏÑú ref_raw_idÎ•º ÌÜµÌï¥ title, snippet Ï°∞Ïù∏
        # ÎßåÏïΩ force_all=FalseÏù¥Î©¥, llm_decisionÏù¥ NULLÏù∏ Í≤ÉÎßå Ï°∞Ìöå
        where_clause = "" if force_all else "WHERE p.llm_decision IS NULL"
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        # SQLite vs Others
        query = f"""
            SELECT p.id, r.title
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            {where_clause}
            ORDER BY p.id DESC
            {limit_clause}
        """
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        articles = []
        for row in rows:
            articles.append({
                "id": row[0],
                "title": row[1]
            })
            
        return articles
    except Exception as e:
        logger.error(f"‚ùå Failed to fetch articles: {e}")
        return []

def call_llm_batch(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """LLM Î∞∞Ïπò Ìò∏Ï∂ú"""
    if not articles:
        return []
    
    # Prompt Íµ¨ÏÑ±
    system_prompt = get_p4_topic_classification_prompt()
    
    # User Content: JSON Array of articles (Title-only for efficiency)
    user_content_data = [
        {"id": str(a["id"]), "title": a["title"]} 
        for a in articles
    ]
    user_content = json.dumps(user_content_data, ensure_ascii=False, indent=2)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here are the articles to classify:\n{user_content}"}
            ],
            temperature=0.0,
            response_format={"type": "json_object"} # JSON Î™®Îìú
        )
        
        # Parse Response
        response_text = response.choices[0].message.content
        parsed_data = json.loads(response_text)
        
        # GPTÍ∞Ä Í∞ÄÎÅî {"articles": [...]} ÌòïÌÉúÎ°ú Ï§Ñ ÎïåÎèÑ ÏûàÍ≥† Î∞îÎ°ú [...] Ï§Ñ ÎïåÎèÑ ÏûàÏùå.
        # ÌîÑÎ°¨ÌîÑÌä∏Îäî ArrayÎ•º ÏöîÍµ¨ÌñàÏúºÎÇò, json_object Î™®ÎìúÎäî root objectÎ•º Í∞ïÏ†úÌïòÍ∏∞ÎèÑ Ìï®.
        # Îî∞ÎùºÏÑú Î≥¥ÌÜµ {"results": [...]} Í∞ôÏùÄ ÎûòÌçºÎ•º Ïì∞Í±∞ÎÇò ÏùëÎãµÏùÑ Ïú†Ïó∞ÌïòÍ≤å Ï≤òÎ¶¨.
        
        # Ïó¨Í∏∞ÏÑúÎäî ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ArrayÎ•º Î¶¨ÌÑ¥ÌïòÎùºÍ≥† Í∞ïÎ†•Ìûà ÏßÄÏãúÌñàÏßÄÎßå, 
        # API response_format={"type": "json_object"}Î•º Ïì∞Î©¥ Î∞òÎìúÏãú {}Î°ú Í∞êÏã∏Ïïº Ïò§Î•òÍ∞Ä ÏïàÎÇ®.
        # Îî∞ÎùºÏÑú ÌîÑÎ°¨ÌîÑÌä∏ ÏàòÏ†ï ÌòπÏùÄ ÌõÑÏ≤òÎ¶¨ ÌïÑÏöî.
        # -> ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑú "Return ONLY a JSON Array"ÎùºÍ≥† ÌñàÏßÄÎßå json_object Î™®ÎìúÎäî { key: value }Î•º ÏöîÍµ¨Ìï®.
        # ÏïàÏ†ÑÌïòÍ≤åÎäî response_formatÏùÑ ÎπºÍ±∞ÎÇò, ÌîÑÎ°¨ÌîÑÌä∏Î•º { "results": [ ... ] }Î°ú Î∞îÍæ∏ÎäîÍ≤å Ï†ïÏÑù.
        # ÏùºÎã® P4 ÌîÑÎ°¨ÌîÑÌä∏Îäî ArrayÎ•º ÏöîÍµ¨ÌïòÎØÄÎ°ú response_formatÏùÑ Î∫çÎãàÎã§. (gpt-4oÎäî Ïûò ÏïåÏïÑÎì§Ïùå)
        # ÌòπÏùÄ Text Î™®ÎìúÎ°ú Î∞õÍ≥† ÌååÏã±.
        
        # ÎßåÏïΩ response_format={"type": "json_object"}Î•º ÏçºÎã§Î©¥ ÏóêÎü¨ ÎÇ¨ÏùÑ Ïàò ÏûàÏùå.
        # ÏúÑ ÏΩîÎìúÏóêÏÑú response_formatÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÏßÑÌñâÌïòÍ±∞ÎÇò,
        # ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÎßûÏ∂§. Ïó¨Í∏∞ÏÑúÎäî response_format ÏóÜÏù¥ ÏßÑÌñâÌïòÍ≤†ÏäµÎãàÎã§.
        
        if isinstance(parsed_data, list):
            return parsed_data
        elif isinstance(parsed_data, dict):
            # ÌòπÏãú ÌÇ§Í∞í ÏïàÏóê Î¶¨Ïä§Ìä∏Í∞Ä ÏûàÎã§Î©¥
            for key in parsed_data:
                if isinstance(parsed_data[key], list):
                    return parsed_data[key]
            # ÏóÜÎã§Î©¥ Îã®Ïùº Í∞ùÏ≤¥ÏùºÏàòÎèÑ?
            return [parsed_data]
            
        return []
        
    except json.JSONDecodeError:
        logger.error("‚ùå LLM output is not valid JSON")
        logger.debug(f"Output: {response_text}")
        return []
    except Exception as e:
        logger.error(f"‚ùå LLM Call Failed: {e}")
        return []

# Defined 9 Categories + Validation
VALID_CATEGORIES = {
    "G_mac", "G_mak", "G_tech", "G_re", 
    "Real_G", "Real_K", 
    "K_mac", "K_mak", "K_in"
}

def call_llm_batch_no_json_mode(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    system_prompt = get_p4_topic_classification_prompt()
    
    # Payload Optimization:
    # 1. Rename keys: id -> i, title -> t
    # 2. Remove snippet (title only sufficient per user request)
    user_content_data = [
        {"i": str(a["id"]), "t": a["title"]} 
        for a in articles
    ]
    # 3. Minified JSON: separators=(',', ':') removes whitespace
    user_content = json.dumps(user_content_data, ensure_ascii=False, separators=(',', ':'))
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.0
        )
        content = response.choices[0].message.content
        
        # Markdown backticks Ï†úÍ±∞ (```json ... ```)
        if "```" in content:
            content = content.replace("```json", "").replace("```", "").strip()
        
        # Parse Response (Array of Arrays)
        # Expected: [[id, decision_bool, category, reason], ...]
        # Robust JSON Extraction: Find first '[' and last ']'
        # Robust JSON Extraction: Find first '['
        try:
            start_idx = content.find('[')
            if start_idx == -1:
                # No list found
                logger.warning(f"‚ö†Ô∏è No JSON list found in response. Raw content: {content[:100]}...")
                return []
            
            # Use raw_decode to parse starting from the first bracket
            # This handles cases where there is extra text/data after the valid JSON
            json_str = content[start_idx:]
            raw_list, _ = json.JSONDecoder().raw_decode(json_str)
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON Decode Error: {e} | Content Snippet: {content[:100]}...")
            return []
        
        parsed_results = []
        for item in raw_list:
            # Robust parsing: handle both old dict style (just in case) and new list style
            if isinstance(item, list) and len(item) >= 4:
                # [ID, DECISION_BOOL, CATEGORY, REASON]
                p_id = item[0]
                dec_bool = item[1]
                cat = item[2]
                reason = item[3]
                
                # Validation Logic: Hallucination Check
                if cat not in VALID_CATEGORIES:
                    if str(dec_bool) == "1" and cat != "Noise":
                        logger.warning(f"‚ö†Ô∏è Hallucination detected: Category '{cat}' is invalid. Forcing DROP (ID: {p_id}).")
                    decision = "DROP"
                else:
                    decision = "KEEP" if str(dec_bool) == "1" or str(dec_bool).lower() == "true" else "DROP"
                
                parsed_results.append({
                    "id": p_id,
                    "decision": decision,
                    "category": cat,
                    "reason": reason
                })
            elif isinstance(item, dict):
                # Fallback for dict (should mostly not happen with new prompt)
                parsed_results.append(item)
                
        return parsed_results
        
    except Exception as e:
        logger.error(f"‚ùå LLM Call Failed: {e}")
        return []

def export_to_gsheet(run_stats: Dict[str, int], sheet_id: str, db: DatabaseAdapter):
    """Í≤∞Í≥ºÎ•º Google SheetsÏóê Ï∂úÎ†•"""
    if not sheet_id:
        return
    
    tab_name = "4.llm_classification"
    
    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        # ÏµúÍ∑º Ï≤òÎ¶¨Îêú Ìï≠Î™©Îì§ Ï°∞Ìöå (limit) - ÌòπÏùÄ Ï†ÑÏ≤¥ Ï°∞Ìöå
        cursor = db.connection.cursor()
        
        # 1. Total Remaining in DB (KEEP only)
        # Note: DROP items are deleted at the end of script, so current DB might still have them if called before cleanup
        # But we want to show "Effective Saved".
        if DB_TYPE == "sqlite":
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        else:
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        
        total_saved_in_db = cursor.fetchone()[0]
        
        # 2. Get details for sheet
        query = """
            SELECT p.id, p.llm_decision, p.llm_category, p.llm_reason, r.title, COALESCE(r.publisher, r.source) as source
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            WHERE p.llm_decision IS NOT NULL
            ORDER BY p.id DESC
            LIMIT 1000
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        
        sheet_rows = []
        sheet_rows.append(["=== Phase 4 LLM Classification Results (Last 1000) ===", "", "", "", "", ""])
        sheet_rows.append(["ID", "Decision", "Category", "Reason", "Title", "Publisher"])
        
        for row in rows:
            p_id, dec, cat, rea, title, src = row
            # dec might be None if query logic changes, but here it's filtered
            sheet_rows.append([p_id, dec, cat, rea, title[:50] if title else "", src])
                
        # ÏÉÅÎã®Ïóê ÌÜµÍ≥Ñ Ï∂îÍ∞Ä (User Request Format)
        # "Total Processed 000, KEEP: 000, DROP: 000 | Total Saved in DB: 000"
        
        stats_str = f"Current Run: {run_stats['processed']} (KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']})"
        db_str = f"Total Saved in DB: {total_saved_in_db}"
        
        sheet_rows.insert(1, [stats_str, "", db_str, "", "", ""])
        sheet_rows.insert(2, ["", "", "", "", "", ""])

        if sheet_rows:
            adapter.worksheet.insert_rows(sheet_rows, 1)
        
        logger.info(f"‚úÖ Exported {len(rows)} rows to sheet '{tab_name}'")
        
    except Exception as e:
        logger.error(f"‚ùå Sheet Export Failed: {e}")

def main():
    parser = argparse.ArgumentParser(description="Phase 4: LLM Classification")
    parser.add_argument("--limit", type=int, help="Ï≤òÎ¶¨Ìï† Í∏∞ÏÇ¨ Ïàò Ï†úÌïú")
    parser.add_argument("--batch-size", type=int, default=100, help="LLM Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à")
    parser.add_argument("--force-all", action="store_true", help="Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Í∏∞ÏÇ¨ÎèÑ Îã§Ïãú Ï≤òÎ¶¨")
    parser.add_argument("--no-export", action="store_true", help="Sheet Ï∂úÎ†• Í±¥ÎÑàÎõ∞Í∏∞")
    args = parser.parse_args()

    # OpenAI Client
    if not OPENAI_AVAILABLE or not OPENAI_API_KEY:
        logger.error("‚ùå OpenAI API Key is missing. Cannot proceed.")
        return
    
    client = OpenAI(api_key=OPENAI_API_KEY)

    logger.info("\n" + "="*80)
    logger.info("üöÄ Phase 4: LLM Classification Start")
    logger.info("="*80)

    # DB Ïó∞Í≤∞
    try:
        db = DatabaseAdapter(
            db_type=DB_TYPE,
            host=DB_HOST if DB_TYPE != "sqlite" else None,
            port=DB_PORT if DB_TYPE != "sqlite" else None,
            database=DB_NAME,
            user=DB_USER if DB_TYPE != "sqlite" else None,
            password=DB_PASSWORD if DB_TYPE != "sqlite" else None
        )
        db.connect()
        # Ïª¨Îüº ÌôïÏù∏ Î∞è Ï∂îÍ∞Ä
        db.ensure_llm_columns()
        db.ensure_publisher_column()
    except Exception as e:
        logger.error(f"‚ùå DB Init Failed: {e}")
        return

    # 1. ÎåÄÏÉÅ Í∏∞ÏÇ¨ Í∞ÄÏ†∏Ïò§Í∏∞
    articles = get_articles_to_process(db, limit=args.limit, force_all=args.force_all)
    total_articles = len(articles)
    logger.info(f"üì• Processing {total_articles} articles (Batch Size: {args.batch_size})")
    
    # 2. Î∞∞Ïπò Ï≤òÎ¶¨
    run_stats = {"processed": 0, "KEEP": 0, "DROP": 0}
    
    for i in range(0, total_articles, args.batch_size):
        batch = articles[i:i + args.batch_size]
        logger.info(f"ü§ñ Processing Batch {i//args.batch_size + 1} ({len(batch)} articles)...")
        
        llm_results = call_llm_batch_no_json_mode(client, batch)
        
        if llm_results:
            # DB ÏóÖÎç∞Ïù¥Ìä∏
            updated = db.update_llm_results(llm_results)
            
            # Stats Counting
            for res in llm_results:
                run_stats["processed"] += 1
                dec = res.get("decision", "DROP").upper() # Default to DROP if missing (safety)
                if dec == "KEEP":
                    run_stats["KEEP"] += 1
                elif dec == "DROP":
                    run_stats["DROP"] += 1
                # else: ignore or count as DROP
        else:
            logger.warning("‚ö†Ô∏è Empty results from LLM batch.")
            
    logger.info(f"‚úÖ Completed. {run_stats['processed']} processed (KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']})")
    
    # Stats Collection
    try:
        from src.utils.stats_collector import StatsCollector
        sc = StatsCollector()
        sc.set_stat("llm_keep", run_stats['KEEP'])
        sc.set_stat("llm_drop", run_stats['DROP'])
    except Exception as e:
        logger.error(f"Stats collection failed: {e}")
    
    
    # 3. Í≤∞Í≥º Ï∂úÎ†• (ÏÉùÎûµ)
    # if not args.no_export:
    #     export_to_gsheet(run_stats, GOOGLE_SHEET_ID, db)
    pass

    # 4. DROP Í∏∞ÏÇ¨ ÏÇ≠Ï†ú (Cleanup)
    delete_dropped_articles(db)

    db.close()

def delete_dropped_articles(db: DatabaseAdapter):
    """LLMÏù¥ 'DROP'ÏúºÎ°ú ÌåêÏ†ïÌïú Í∏∞ÏÇ¨Î•º DBÏóêÏÑú ÏòÅÍµ¨ ÏÇ≠Ï†ú"""
    logger.info("\n" + "="*60)
    logger.info("üóëÔ∏è Cleanup: Deleting 'DROP' articles...")
    logger.info("="*60)
    
    try:
        cursor = db.connection.cursor()
        
        # 1. ÏÇ≠Ï†úÌï† ÎåÄÏÉÅ Ï°∞Ìöå (processed_news)
        # DROPÏù∏ Ìï≠Î™©Ïùò idÏôÄ ref_raw_idÎ•º Ï°∞Ìöå
        if DB_TYPE == "sqlite":
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
        else:
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
            
        cursor.execute(query_select)
        rows = cursor.fetchall()
        
        if not rows:
            logger.info("‚ÑπÔ∏è No 'DROP' articles found to delete.")
            return

        p_ids = [row[0] for row in rows]
        r_ids = [row[1] for row in rows if row[1] is not None]
        
        logger.info(f"Found {len(p_ids)} articles marked as DROP.")
        
        # 2. processed_newsÏóêÏÑú ÏÇ≠Ï†ú
        if DB_TYPE == "sqlite":
             placeholders_p = ",".join("?" * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
        else:
             placeholders_p = ",".join(["%s"] * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
             
        p_deleted = cursor.rowcount
        
        # 3. raw_newsÏóêÏÑú ÏÇ≠Ï†ú
        if r_ids:
            if DB_TYPE == "sqlite":
                placeholders_r = ",".join("?" * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            else:
                placeholders_r = ",".join(["%s"] * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            r_deleted = cursor.rowcount
        else:
            r_deleted = 0
            
        db.connection.commit()
        logger.info(f"‚úÖ Cleanup Complete: Deleted {p_deleted} processed_news rows and {r_deleted} raw_news rows.")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to delete DROP articles: {e}")
        db.connection.rollback()


if __name__ == "__main__":
    main()
