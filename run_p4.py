#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 4: LLM Classification (run_p4.py)

ê¸°ëŠ¥:
1. processed_news í…Œì´ë¸”ì—ì„œ ê¸°ì‚¬ ì¡°íšŒ
2. LLM(GPT-4o)ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ 9ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
3. ë¶„ë¥˜ ê²°ê³¼ (Decision, Category, Reason)ë¥¼ DBì— ì €ì¥
4. ê²°ê³¼ í†µê³„ë¥¼ Google Sheetì— ì¶œë ¥ (4.llm_classification)

Usage:
    python run_p4.py
"""

import sys
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from config.settings import (
    DB_TYPE, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,
    GOOGLE_SHEET_ID, LOG_LEVEL, OPENAI_API_KEY
)
from config.prompts.classification_pt import get_p4_topic_classification_prompt

# OpenAI í´ë¼ì´ì–¸íŠ¸ (Phase 2ì™€ ë™ì¼í•˜ê²Œ ì²´í¬)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = setup_logger(log_level=LOG_LEVEL)

def get_articles_to_process(db: DatabaseAdapter, limit: int = None, force_all: bool = False) -> List[Dict[str, Any]]:
    """ì²˜ë¦¬í•  ê¸°ì‚¬ ì¡°íšŒ"""
    try:
        cursor = db.connection.cursor()
        
        # 1. processed_newsì—ì„œ ref_raw_idë¥¼ í†µí•´ title, snippet ì¡°ì¸
        # ë§Œì•½ force_all=Falseì´ë©´, llm_decisionì´ NULLì¸ ê²ƒë§Œ ì¡°íšŒ
        where_clause = "" if force_all else "WHERE p.llm_decision IS NULL"
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        # SQLite vs Others
        query = f"""
            SELECT p.id, r.title, r.snippet
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            {where_clause}
            ORDER BY p.id DESC
            {limit_clause}
        """
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        articles = []
        for row in rows:
            articles.append({
                "id": row[0],
                "title": row[1],
                "snippet": row[2] or ""
            })
            
        return articles
    except Exception as e:
        logger.error(f"âŒ Failed to fetch articles: {e}")
        return []

def call_llm_batch(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """LLM ë°°ì¹˜ í˜¸ì¶œ"""
    if not articles:
        return []
    
    # Prompt êµ¬ì„±
    system_prompt = get_p4_topic_classification_prompt()
    
    # User Content: JSON Array of articles
    user_content_data = [
        {"id": str(a["id"]), "title": a["title"], "snippet": a["snippet"]} 
        for a in articles
    ]
    user_content = json.dumps(user_content_data, ensure_ascii=False, indent=2)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here are the articles to classify:\n{user_content}"}
            ],
            temperature=0.0,
            response_format={"type": "json_object"} # JSON ëª¨ë“œ
        )
        
        # Parse Response
        response_text = response.choices[0].message.content
        parsed_data = json.loads(response_text)
        
        # GPTê°€ ê°€ë” {"articles": [...]} í˜•íƒœë¡œ ì¤„ ë•Œë„ ìˆê³  ë°”ë¡œ [...] ì¤„ ë•Œë„ ìˆìŒ.
        # í”„ë¡¬í”„íŠ¸ëŠ” Arrayë¥¼ ìš”êµ¬í–ˆìœ¼ë‚˜, json_object ëª¨ë“œëŠ” root objectë¥¼ ê°•ì œí•˜ê¸°ë„ í•¨.
        # ë”°ë¼ì„œ ë³´í†µ {"results": [...]} ê°™ì€ ë˜í¼ë¥¼ ì“°ê±°ë‚˜ ì‘ë‹µì„ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬.
        
        # ì—¬ê¸°ì„œëŠ” í”„ë¡¬í”„íŠ¸ê°€ Arrayë¥¼ ë¦¬í„´í•˜ë¼ê³  ê°•ë ¥íˆ ì§€ì‹œí–ˆì§€ë§Œ, 
        # API response_format={"type": "json_object"}ë¥¼ ì“°ë©´ ë°˜ë“œì‹œ {}ë¡œ ê°ì‹¸ì•¼ ì˜¤ë¥˜ê°€ ì•ˆë‚¨.
        # ë”°ë¼ì„œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • í˜¹ì€ í›„ì²˜ë¦¬ í•„ìš”.
        # -> í”„ë¡¬í”„íŠ¸ì—ì„œ "Return ONLY a JSON Array"ë¼ê³  í–ˆì§€ë§Œ json_object ëª¨ë“œëŠ” { key: value }ë¥¼ ìš”êµ¬í•¨.
        # ì•ˆì „í•˜ê²ŒëŠ” response_formatì„ ë¹¼ê±°ë‚˜, í”„ë¡¬í”„íŠ¸ë¥¼ { "results": [ ... ] }ë¡œ ë°”ê¾¸ëŠ”ê²Œ ì •ì„.
        # ì¼ë‹¨ P4 í”„ë¡¬í”„íŠ¸ëŠ” Arrayë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ response_formatì„ ëºë‹ˆë‹¤. (gpt-4oëŠ” ì˜ ì•Œì•„ë“¤ìŒ)
        # í˜¹ì€ Text ëª¨ë“œë¡œ ë°›ê³  íŒŒì‹±.
        
        # ë§Œì•½ response_format={"type": "json_object"}ë¥¼ ì¼ë‹¤ë©´ ì—ëŸ¬ ë‚¬ì„ ìˆ˜ ìˆìŒ.
        # ìœ„ ì½”ë“œì—ì„œ response_formatì„ ì œê±°í•˜ê³  ì§„í–‰í•˜ê±°ë‚˜,
        # í”„ë¡¬í”„íŠ¸ì™€ ë§ì¶¤. ì—¬ê¸°ì„œëŠ” response_format ì—†ì´ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
        
        if isinstance(parsed_data, list):
            return parsed_data
        elif isinstance(parsed_data, dict):
            # í˜¹ì‹œ í‚¤ê°’ ì•ˆì— ë¦¬ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´
            for key in parsed_data:
                if isinstance(parsed_data[key], list):
                    return parsed_data[key]
            # ì—†ë‹¤ë©´ ë‹¨ì¼ ê°ì²´ì¼ìˆ˜ë„?
            return [parsed_data]
            
        return []
        
    except json.JSONDecodeError:
        logger.error("âŒ LLM output is not valid JSON")
        logger.debug(f"Output: {response_text}")
        return []
    except Exception as e:
        logger.error(f"âŒ LLM Call Failed: {e}")
        return []

# ì¬ì •ì˜: response_format ì—†ì´ í˜¸ì¶œí•˜ëŠ” ë²„ì „ (Array ë°˜í™˜ì„ ìœ„í•´)
def call_llm_batch_no_json_mode(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    system_prompt = get_p4_topic_classification_prompt()
    user_content_data = [
        {"id": str(a["id"]), "title": a["title"], "snippet": a["snippet"][:200]} # ìŠ¤ë‹ˆí« 200ìë¡œ ì œí•œ 
        for a in articles
    ]
    user_content = json.dumps(user_content_data, ensure_ascii=False)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.0
        )
        content = response.choices[0].message.content
        
        # Markdown backticks ì œê±° (```json ... ```)
        if "```" in content:
            content = content.replace("```json", "").replace("```", "").strip()
            
        return json.loads(content)
    except Exception as e:
        logger.error(f"âŒ LLM Call Failed: {e}")
        return []

def export_to_gsheet(run_stats: Dict[str, int], sheet_id: str, db: DatabaseAdapter):
    """ê²°ê³¼ë¥¼ Google Sheetsì— ì¶œë ¥"""
    if not sheet_id:
        return
    
    tab_name = "4.llm_classification"
    
    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        # ìµœê·¼ ì²˜ë¦¬ëœ í•­ëª©ë“¤ ì¡°íšŒ (limit) - í˜¹ì€ ì „ì²´ ì¡°íšŒ
        cursor = db.connection.cursor()
        
        # 1. Total Remaining in DB (KEEP only)
        # Note: DROP items are deleted at the end of script, so current DB might still have them if called before cleanup
        # But we want to show "Effective Saved".
        if DB_TYPE == "sqlite":
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        else:
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        
        total_saved_in_db = cursor.fetchone()[0]
        
        # 2. Get details for sheet
        query = """
            SELECT p.id, p.llm_decision, p.llm_category, p.llm_reason, r.title, COALESCE(r.publisher, r.source) as source
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            WHERE p.llm_decision IS NOT NULL
            ORDER BY p.id DESC
            LIMIT 1000
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        
        sheet_rows = []
        sheet_rows.append(["=== Phase 4 LLM Classification Results (Last 1000) ===", "", "", "", "", ""])
        sheet_rows.append(["ID", "Decision", "Category", "Reason", "Title", "Publisher"])
        
        for row in rows:
            p_id, dec, cat, rea, title, src = row
            # dec might be None if query logic changes, but here it's filtered
            sheet_rows.append([p_id, dec, cat, rea, title[:50] if title else "", src])
                
        # ìƒë‹¨ì— í†µê³„ ì¶”ê°€ (User Request Format)
        # "Total Processed 000, KEEP: 000, DROP: 000 | Total Saved in DB: 000"
        
        stats_str = f"Current Run: {run_stats['processed']} (KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']})"
        db_str = f"Total Saved in DB: {total_saved_in_db}"
        
        sheet_rows.insert(1, [stats_str, "", db_str, "", "", ""])
        sheet_rows.insert(2, ["", "", "", "", "", ""])

        if sheet_rows:
            adapter.worksheet.insert_rows(sheet_rows, 1)
        
        logger.info(f"âœ… Exported {len(rows)} rows to sheet '{tab_name}'")
        
    except Exception as e:
        logger.error(f"âŒ Sheet Export Failed: {e}")

def main():
    parser = argparse.ArgumentParser(description="Phase 4: LLM Classification")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ì œí•œ")
    parser.add_argument("--batch-size", type=int, default=25, help="LLM ë°°ì¹˜ ì‚¬ì´ì¦ˆ")
    parser.add_argument("--force-all", action="store_true", help="ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬ë„ ë‹¤ì‹œ ì²˜ë¦¬")
    parser.add_argument("--no-export", action="store_true", help="Sheet ì¶œë ¥ ê±´ë„ˆë›°ê¸°")
    args = parser.parse_args()

    # OpenAI Client
    if not OPENAI_AVAILABLE or not OPENAI_API_KEY:
        logger.error("âŒ OpenAI API Key is missing. Cannot proceed.")
        return
    
    client = OpenAI(api_key=OPENAI_API_KEY)

    logger.info("\n" + "="*80)
    logger.info("ğŸš€ Phase 4: LLM Classification Start")
    logger.info("="*80)

    # DB ì—°ê²°
    try:
        db = DatabaseAdapter(
            db_type=DB_TYPE,
            host=DB_HOST if DB_TYPE != "sqlite" else None,
            port=DB_PORT if DB_TYPE != "sqlite" else None,
            database=DB_NAME,
            user=DB_USER if DB_TYPE != "sqlite" else None,
            password=DB_PASSWORD if DB_TYPE != "sqlite" else None
        )
        db.connect()
        # ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
        db.ensure_llm_columns()
        db.ensure_publisher_column()
    except Exception as e:
        logger.error(f"âŒ DB Init Failed: {e}")
        return

    # 1. ëŒ€ìƒ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    articles = get_articles_to_process(db, limit=args.limit, force_all=args.force_all)
    total_articles = len(articles)
    logger.info(f"ğŸ“¥ Processing {total_articles} articles (Batch Size: {args.batch_size})")
    
    # 2. ë°°ì¹˜ ì²˜ë¦¬
    run_stats = {"processed": 0, "KEEP": 0, "DROP": 0}
    
    for i in range(0, total_articles, args.batch_size):
        batch = articles[i:i + args.batch_size]
        logger.info(f"ğŸ¤– Processing Batch {i//args.batch_size + 1} ({len(batch)} articles)...")
        
        llm_results = call_llm_batch_no_json_mode(client, batch)
        
        if llm_results:
            # DB ì—…ë°ì´íŠ¸
            updated = db.update_llm_results(llm_results)
            
            # Stats Counting
            for res in llm_results:
                run_stats["processed"] += 1
                dec = res.get("decision", "DROP").upper() # Default to DROP if missing (safety)
                if dec == "KEEP":
                    run_stats["KEEP"] += 1
                elif dec == "DROP":
                    run_stats["DROP"] += 1
                # else: ignore or count as DROP
        else:
            logger.warning("âš ï¸ Empty results from LLM batch.")
            
    logger.info(f"âœ… Completed. {run_stats['processed']} processed (KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']})")
    
    # Stats Collection
    try:
        from src.utils.stats_collector import StatsCollector
        sc = StatsCollector()
        sc.set_stat("llm_keep", run_stats['KEEP'])
        sc.set_stat("llm_drop", run_stats['DROP'])
    except Exception as e:
        logger.error(f"Stats collection failed: {e}")
    
    
    # 3. ê²°ê³¼ ì¶œë ¥ (ìƒëµ)
    # if not args.no_export:
    #     export_to_gsheet(run_stats, GOOGLE_SHEET_ID, db)
    pass

    # 4. DROP ê¸°ì‚¬ ì‚­ì œ (Cleanup)
    delete_dropped_articles(db)

    db.close()

def delete_dropped_articles(db: DatabaseAdapter):
    """LLMì´ 'DROP'ìœ¼ë¡œ íŒì •í•œ ê¸°ì‚¬ë¥¼ DBì—ì„œ ì˜êµ¬ ì‚­ì œ"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ—‘ï¸ Cleanup: Deleting 'DROP' articles...")
    logger.info("="*60)
    
    try:
        cursor = db.connection.cursor()
        
        # 1. ì‚­ì œí•  ëŒ€ìƒ ì¡°íšŒ (processed_news)
        # DROPì¸ í•­ëª©ì˜ idì™€ ref_raw_idë¥¼ ì¡°íšŒ
        if DB_TYPE == "sqlite":
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
        else:
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
            
        cursor.execute(query_select)
        rows = cursor.fetchall()
        
        if not rows:
            logger.info("â„¹ï¸ No 'DROP' articles found to delete.")
            return

        p_ids = [row[0] for row in rows]
        r_ids = [row[1] for row in rows if row[1] is not None]
        
        logger.info(f"Found {len(p_ids)} articles marked as DROP.")
        
        # 2. processed_newsì—ì„œ ì‚­ì œ
        if DB_TYPE == "sqlite":
             placeholders_p = ",".join("?" * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
        else:
             placeholders_p = ",".join(["%s"] * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
             
        p_deleted = cursor.rowcount
        
        # 3. raw_newsì—ì„œ ì‚­ì œ
        if r_ids:
            if DB_TYPE == "sqlite":
                placeholders_r = ",".join("?" * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            else:
                placeholders_r = ",".join(["%s"] * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            r_deleted = cursor.rowcount
        else:
            r_deleted = 0
            
        db.connection.commit()
        logger.info(f"âœ… Cleanup Complete: Deleted {p_deleted} processed_news rows and {r_deleted} raw_news rows.")
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete DROP articles: {e}")
        db.connection.rollback()


if __name__ == "__main__":
    main()
