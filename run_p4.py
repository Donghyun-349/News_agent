#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 4: LLM Classification (run_p4.py)

ê¸°ëŠ¥:
1. processed_news í…Œì´ë¸”ì—ì„œ ê¸°ì‚¬ ì¡°íšŒ
2. LLM(GPT-4o)ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ 9ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
3. ë¶„ë¥˜ ê²°ê³¼ (Decision, Category, Reason)ë¥¼ DBì— ì €ì¥
4. ê²°ê³¼ í†µê³„ë¥¼ Google Sheetì— ì¶œë ¥ (4.llm_classification)

Usage:
    python run_p4.py
"""

import sys
import json
import re
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from config.settings import (
    DB_TYPE, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,
    GOOGLE_SHEET_ID, LOG_LEVEL, OPENAI_API_KEY
)
from config.prompts.classification_pt import get_p4_topic_classification_prompt

# OpenAI í´ë¼ì´ì–¸íŠ¸ (Phase 2ì™€ ë™ì¼í•˜ê²Œ ì²´í¬)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = setup_logger(log_level=LOG_LEVEL)

def get_articles_to_process(db: DatabaseAdapter, limit: int = None, force_all: bool = False) -> List[Dict[str, Any]]:
    """ì²˜ë¦¬í•  ê¸°ì‚¬ ì¡°íšŒ"""
    try:
        cursor = db.connection.cursor()
        
        # 1. processed_newsì—ì„œ ref_raw_idë¥¼ í†µí•´ title, snippet ì¡°ì¸
        # ë§Œì•½ force_all=Falseì´ë©´, llm_decisionì´ NULL ë˜ëŠ” ERRORì¸ ê²ƒë§Œ ì¡°íšŒ (Two-Pass)
        where_clause = "" if force_all else "WHERE (p.llm_decision IS NULL OR p.llm_decision = 'ERROR')"
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        # SQLite vs Others
        query = f"""
            SELECT p.id, r.title
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            {where_clause}
            ORDER BY p.id DESC
            {limit_clause}
        """
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        articles = []
        for row in rows:
            articles.append({
                "id": row[0],
                "title": row[1]
            })
            
        return articles
    except Exception as e:
        logger.error(f"âŒ Failed to fetch articles: {e}")
        return []

def call_llm_batch(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """LLM ë°°ì¹˜ í˜¸ì¶œ"""
    if not articles:
        return []
    
    # Prompt êµ¬ì„±
    system_prompt = get_p4_topic_classification_prompt()
    
    # User Content: JSON Array of articles (Title-only for efficiency)
    user_content_data = [
        {"id": str(a["id"]), "title": a["title"]} 
        for a in articles
    ]
    user_content = json.dumps(user_content_data, ensure_ascii=False, indent=2)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here are the articles to classify:\n{user_content}"}
            ],
            temperature=0.0,
            response_format={"type": "json_object"} # JSON ëª¨ë“œ
        )
        
        # Parse Response
        response_text = response.choices[0].message.content
        parsed_data = json.loads(response_text)
        
        # GPTê°€ ê°€ë” {"articles": [...]} í˜•íƒœë¡œ ì¤„ ë•Œë„ ìˆê³  ë°”ë¡œ [...] ì¤„ ë•Œë„ ìˆìŒ.
        # í”„ë¡¬í”„íŠ¸ëŠ” Arrayë¥¼ ìš”êµ¬í–ˆìœ¼ë‚˜, json_object ëª¨ë“œëŠ” root objectë¥¼ ê°•ì œí•˜ê¸°ë„ í•¨.
        # ë”°ë¼ì„œ ë³´í†µ {"results": [...]} ê°™ì€ ë˜í¼ë¥¼ ì“°ê±°ë‚˜ ì‘ë‹µì„ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬.
        
        # ì—¬ê¸°ì„œëŠ” í”„ë¡¬í”„íŠ¸ê°€ Arrayë¥¼ ë¦¬í„´í•˜ë¼ê³  ê°•ë ¥íˆ ì§€ì‹œí–ˆì§€ë§Œ, 
        # API response_format={"type": "json_object"}ë¥¼ ì“°ë©´ ë°˜ë“œì‹œ {}ë¡œ ê°ì‹¸ì•¼ ì˜¤ë¥˜ê°€ ì•ˆë‚¨.
        # ë”°ë¼ì„œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • í˜¹ì€ í›„ì²˜ë¦¬ í•„ìš”.
        # -> í”„ë¡¬í”„íŠ¸ì—ì„œ "Return ONLY a JSON Array"ë¼ê³  í–ˆì§€ë§Œ json_object ëª¨ë“œëŠ” { key: value }ë¥¼ ìš”êµ¬í•¨.
        # ì•ˆì „í•˜ê²ŒëŠ” response_formatì„ ë¹¼ê±°ë‚˜, í”„ë¡¬í”„íŠ¸ë¥¼ { "results": [ ... ] }ë¡œ ë°”ê¾¸ëŠ”ê²Œ ì •ì„.
        # ì¼ë‹¨ P4 í”„ë¡¬í”„íŠ¸ëŠ” Arrayë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ response_formatì„ ëºë‹ˆë‹¤. (gpt-4oëŠ” ì˜ ì•Œì•„ë“¤ìŒ)
        # í˜¹ì€ Text ëª¨ë“œë¡œ ë°›ê³  íŒŒì‹±.
        
        # ë§Œì•½ response_format={"type": "json_object"}ë¥¼ ì¼ë‹¤ë©´ ì—ëŸ¬ ë‚¬ì„ ìˆ˜ ìˆìŒ.
        # ìœ„ ì½”ë“œì—ì„œ response_formatì„ ì œê±°í•˜ê³  ì§„í–‰í•˜ê±°ë‚˜,
        # í”„ë¡¬í”„íŠ¸ì™€ ë§ì¶¤. ì—¬ê¸°ì„œëŠ” response_format ì—†ì´ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
        
        if isinstance(parsed_data, list):
            return parsed_data
        elif isinstance(parsed_data, dict):
            # í˜¹ì‹œ í‚¤ê°’ ì•ˆì— ë¦¬ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´
            for key in parsed_data:
                if isinstance(parsed_data[key], list):
                    return parsed_data[key]
            # ì—†ë‹¤ë©´ ë‹¨ì¼ ê°ì²´ì¼ìˆ˜ë„?
            return [parsed_data]
            
        return []
        
    except json.JSONDecodeError:
        logger.error("âŒ LLM output is not valid JSON")
        logger.debug(f"Output: {response_text}")
        return []
    except Exception as e:
        logger.error(f"âŒ LLM Call Failed: {e}")
        return []

# Defined 9 Categories + Validation
VALID_CATEGORIES = {
    "G_mac", "G_mak", "G_tech", "G_re", 
    "Real_G", "Real_K", 
    "K_mac", "K_mak", "K_in"
}

def call_llm_batch_no_json_mode(client: OpenAI, articles: List[Dict[str, Any]], model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """LLM ë°°ì¹˜ í˜¸ì¶œ (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° Regex Fallback í¬í•¨)"""
    system_prompt = get_p4_topic_classification_prompt()
    
    # Payload Optimization:
    # 1. Rename keys: id -> i, title -> t
    # 2. Remove snippet (title only sufficient per user request)
    user_content_data = [
        {"i": str(a["id"]), "t": a["title"]} 
        for a in articles
    ]
    # 3. Minified JSON: separators=(',', ':') removes whitespace
    user_content = json.dumps(user_content_data, ensure_ascii=False, separators=(',', ':'))
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.0
        )
        content = response.choices[0].message.content
        
        # Markdown backticks ì œê±° (```json ... ```)
        if "```" in content:
            content = content.replace("```json", "").replace("```", "").strip()
        
        # Parse Response (Array of Arrays)
        # Expected: [[id, decision_bool, category, reason], ...]
        # Robust JSON Extraction: Find first '[' and last ']'
        # Robust JSON Extraction: Find first '['
        raw_list = None
        try:
            start_idx = content.find('[')
            if start_idx == -1:
                # No list found
                logger.warning(f"âš ï¸ No JSON list found in response. Trying regex fallback...")
            else:
                # Use raw_decode to parse starting from the first bracket
                # This handles cases where there is extra text/data after the valid JSON
                json_str = content[start_idx:]
                raw_list, _ = json.JSONDecoder().raw_decode(json_str)
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON Decode Error: {e}. Trying regex fallback...")
        
        # Regex Fallback: Extract list items if JSON parsing failed
        if raw_list is None or not raw_list:
            # Pattern: ["id", decision_bool, "category", "reason"]
            # Handles: ["123", 1, "K_mac", "reason text"], ["124", 0, "G_tech", "reason"]
            pattern = r'\[\s*["\']?(\d+)["\']?\s*,\s*([01]|true|false)\s*,\s*["\']([^"\',]+)["\']\s*,\s*["\']([^"\']*)["\']]'
            matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
            
            if matches:
                logger.info(f"âœ… Regex fallback recovered {len(matches)} items")
                raw_list = [[m[0], m[1], m[2], m[3]] for m in matches]
            else:
                logger.error(f"âŒ Both JSON and Regex parsing failed. Content: {content[:200]}...")
                # Return ERROR state for all articles in this batch
                return [{"id": str(a["id"]), "decision": "ERROR", "category": None, "reason": "Parsing failed"} for a in articles]
        
        parsed_results = []
        for item in raw_list:
            # Robust parsing: handle both old dict style (just in case) and new list style
            if isinstance(item, list) and len(item) >= 4:
                # [ID, DECISION_BOOL, CATEGORY, REASON]
                p_id = item[0]
                dec_bool = item[1]
                cat = item[2]
                reason = item[3]
                
                # Validation Logic: Hallucination Check (ê°œì„ ë¨)
                if cat not in VALID_CATEGORIES:
                    # ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ â†’ ERROR ìƒíƒœë¡œ ë§ˆí‚¹ (ì¬ì²˜ë¦¬ ê¸°íšŒ ë¶€ì—¬)
                    logger.warning(f"âš ï¸ Hallucination detected: Category '{cat}' is invalid. Marking as ERROR for retry (ID: {p_id}).")
                    decision = "ERROR"
                else:
                    decision = "KEEP" if str(dec_bool) == "1" or str(dec_bool).lower() == "true" else "DROP"
                
                parsed_results.append({
                    "id": p_id,
                    "decision": decision,
                    "category": cat,
                    "reason": reason
                })
            elif isinstance(item, dict):
                # Fallback for dict (should mostly not happen with new prompt)
                parsed_results.append(item)
                
        return parsed_results
        
    except Exception as e:
        logger.error(f"âŒ LLM Call Failed: {e}")
        return []

def export_to_gsheet(run_stats: Dict[str, int], sheet_id: str, db: DatabaseAdapter):
    """ê²°ê³¼ë¥¼ Google Sheetsì— ì¶œë ¥"""
    if not sheet_id:
        return
    
    tab_name = "4.llm_classification"
    
    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        # ìµœê·¼ ì²˜ë¦¬ëœ í•­ëª©ë“¤ ì¡°íšŒ (limit) - í˜¹ì€ ì „ì²´ ì¡°íšŒ
        cursor = db.connection.cursor()
        
        # 1. Total Remaining in DB (KEEP only)
        # Note: DROP items are deleted at the end of script, so current DB might still have them if called before cleanup
        # But we want to show "Effective Saved".
        if DB_TYPE == "sqlite":
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        else:
            cursor.execute("SELECT COUNT(*) FROM processed_news WHERE llm_decision = 'KEEP'")
        
        total_saved_in_db = cursor.fetchone()[0]
        
        # 2. Get details for sheet
        query = """
            SELECT p.id, p.llm_decision, p.llm_category, p.llm_reason, r.title, COALESCE(r.publisher, r.source) as source
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            WHERE p.llm_decision IS NOT NULL
            ORDER BY p.id DESC
            LIMIT 1000
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        
        sheet_rows = []
        sheet_rows.append(["=== Phase 4 LLM Classification Results (Last 1000) ===", "", "", "", "", ""])
        sheet_rows.append(["ID", "Decision", "Category", "Reason", "Title", "Publisher"])
        
        for row in rows:
            p_id, dec, cat, rea, title, src = row
            # dec might be None if query logic changes, but here it's filtered
            sheet_rows.append([p_id, dec, cat, rea, title[:50] if title else "", src])
                
        # ìƒë‹¨ì— í†µê³„ ì¶”ê°€ (User Request Format)
        # "Total Processed 000, KEEP: 000, DROP: 000 | Total Saved in DB: 000"
        
        stats_str = f"Current Run: {run_stats['processed']} (KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']})"
        db_str = f"Total Saved in DB: {total_saved_in_db}"
        
        sheet_rows.insert(1, [stats_str, "", db_str, "", "", ""])
        sheet_rows.insert(2, ["", "", "", "", "", ""])

        if sheet_rows:
            adapter.worksheet.insert_rows(sheet_rows, 1)
        
        logger.info(f"âœ… Exported {len(rows)} rows to sheet '{tab_name}'")
        
    except Exception as e:
        logger.error(f"âŒ Sheet Export Failed: {e}")

def main():
    parser = argparse.ArgumentParser(description="Phase 4: LLM Classification")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ì œí•œ")
    parser.add_argument("--batch-size", type=int, default=50, help="LLM ë°°ì¹˜ ì‚¬ì´ì¦ˆ (ê¸°ë³¸: 50, ì•ˆì •ì„± ê°œì„ )")
    parser.add_argument("--force-all", action="store_true", help="ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬ë„ ë‹¤ì‹œ ì²˜ë¦¬")
    parser.add_argument("--no-export", action="store_true", help="Sheet ì¶œë ¥ ê±´ë„ˆë›°ê¸°")
    args = parser.parse_args()

    # OpenAI Client
    if not OPENAI_AVAILABLE or not OPENAI_API_KEY:
        logger.error("âŒ OpenAI API Key is missing. Cannot proceed.")
        return
    
    client = OpenAI(api_key=OPENAI_API_KEY)

    logger.info("\n" + "="*80)
    logger.info("ğŸš€ Phase 4: LLM Classification Start")
    logger.info("="*80)

    # DB ì—°ê²°
    try:
        db = DatabaseAdapter(
            db_type=DB_TYPE,
            host=DB_HOST if DB_TYPE != "sqlite" else None,
            port=DB_PORT if DB_TYPE != "sqlite" else None,
            database=DB_NAME,
            user=DB_USER if DB_TYPE != "sqlite" else None,
            password=DB_PASSWORD if DB_TYPE != "sqlite" else None
        )
        db.connect()
        # ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
        db.ensure_llm_columns()
        db.ensure_publisher_column()
    except Exception as e:
        logger.error(f"âŒ DB Init Failed: {e}")
        return

    # 0. ì´ì „ ì‹¤í–‰ì˜ DROP ê¸°ì‚¬ ì‚­ì œ (Option A: Delayed DELETE)
    logger.info("\n" + "="*80)
    logger.info("ğŸ§¹ Cleanup: Deleting DROP articles from previous run...")
    logger.info("="*80)
    delete_dropped_articles(db)

    # 1. ëŒ€ìƒ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (NULL + ERROR)
    articles = get_articles_to_process(db, limit=args.limit, force_all=args.force_all)
    total_articles = len(articles)
    logger.info(f"ğŸ“¥ Processing {total_articles} articles (Batch Size: {args.batch_size})")
    
    # 2. Two-Pass ì²˜ë¦¬
    run_stats = {"processed": 0, "KEEP": 0, "DROP": 0, "ERROR": 0, "RETRY_SUCCESS": 0, "RETRY_FAILED": 0}
    
    # ========== PASS 1: ì´ˆê¸° ì²˜ë¦¬ ==========
    logger.info("\n" + "="*80)
    logger.info("ğŸ”„ PASS 1: Initial Processing")
    logger.info("="*80)
    
    for i in range(0, total_articles, args.batch_size):
        batch = articles[i:i + args.batch_size]
        logger.info(f"ğŸ¤– Processing Batch {i//args.batch_size + 1} ({len(batch)} articles)...")
        
        llm_results = call_llm_batch_no_json_mode(client, batch)
        
        if llm_results:
            # DB ì—…ë°ì´íŠ¸
            updated = db.update_llm_results(llm_results)
            
            # Stats Counting
            for res in llm_results:
                run_stats["processed"] += 1
                dec = res.get("decision", "DROP").upper() # Default to DROP if missing (safety)
                if dec == "KEEP":
                    run_stats["KEEP"] += 1
                elif dec == "DROP":
                    run_stats["DROP"] += 1
                elif dec == "ERROR":
                    run_stats["ERROR"] += 1
        else:
            logger.warning("âš ï¸ Empty results from LLM batch.")
            
    logger.info(f"âœ… Pass 1 Completed. Processed: {run_stats['processed']}, KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']}, ERROR: {run_stats['ERROR']}")
    
    # ========== PASS 2: ERROR ì¬ì²˜ë¦¬ ==========
    if run_stats["ERROR"] > 0:
        logger.info("\n" + "="*80)
        logger.info(f"ğŸ”„ PASS 2: Retrying {run_stats['ERROR']} ERROR articles")
        logger.info("="*80)
        
        # ERROR ìƒíƒœ ê¸°ì‚¬ë§Œ ë‹¤ì‹œ ì¡°íšŒ
        error_articles = get_articles_to_process(db, limit=None, force_all=False)
        # ì´ë¯¸ Pass 1ì—ì„œ ì²˜ë¦¬ëœ ê²ƒë“¤ì´ë¯€ë¡œ, ì‹¤ì œë¡œëŠ” ERRORì¸ ê²ƒë§Œ í•„í„°ë§ë¨ (ì¿¼ë¦¬ ì¡°ê±´ ì°¸ì¡°)
        error_articles = [a for a in error_articles if a["id"] not in [art["id"] for art in articles]]
        
        # ì¬ì¡°íšŒ: ì‹¤ì œë¡œëŠ” DBì—ì„œ ERROR ìƒíƒœì¸ ê²ƒë§Œ ê°€ì ¸ì˜¤ê¸°
        cursor = db.connection.cursor()
        cursor.execute("""
            SELECT p.id, r.title
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
            WHERE p.llm_decision = 'ERROR'
            ORDER BY p.id DESC
        """)
        error_rows = cursor.fetchall()
        error_articles = [{"id": row[0], "title": row[1]} for row in error_rows]
        
        logger.info(f"ğŸ“¥ Found {len(error_articles)} ERROR articles to retry")
        
        for i in range(0, len(error_articles), args.batch_size):
            batch = error_articles[i:i + args.batch_size]
            logger.info(f"ğŸ” Retry Batch {i//args.batch_size + 1} ({len(batch)} articles)...")
            
            llm_results = call_llm_batch_no_json_mode(client, batch)
            
            if llm_results:
                # DB ì—…ë°ì´íŠ¸
                db.update_llm_results(llm_results)
                
                # Retry Stats
                for res in llm_results:
                    dec = res.get("decision", "DROP").upper()
                    if dec == "KEEP":
                        run_stats["KEEP"] += 1
                        run_stats["RETRY_SUCCESS"] += 1
                        run_stats["ERROR"] -= 1
                    elif dec == "DROP":
                        run_stats["DROP"] += 1
                        run_stats["RETRY_SUCCESS"] += 1
                        run_stats["ERROR"] -= 1
                    elif dec == "ERROR":
                        run_stats["RETRY_FAILED"] += 1
        
        logger.info(f"âœ… Pass 2 Completed. Retry Success: {run_stats['RETRY_SUCCESS']}, Still ERROR: {run_stats['ERROR']}")
    
    # ìµœì¢… í†µê³„
    logger.info("\n" + "="*80)
    logger.info(f"âœ… Final Results: Total Processed: {run_stats['processed']}, KEEP: {run_stats['KEEP']}, DROP: {run_stats['DROP']}, ERROR: {run_stats['ERROR']}")
    logger.info("="*80)
    
    # Stats Collection
    try:
        from src.utils.stats_collector import StatsCollector
        sc = StatsCollector()
        sc.set_stat("llm_keep", run_stats['KEEP'])
        sc.set_stat("llm_drop", run_stats['DROP'])
        sc.set_stat("llm_error", run_stats['ERROR'])
    except Exception as e:
        logger.error(f"Stats collection failed: {e}")
    
    # NOTE: í˜„ì¬ ì‹¤í–‰ì˜ DROP ê¸°ì‚¬ëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì‚­ì œë¨)
    # ì´ë¥¼ í†µí•´ ì‚¬í›„ ê²€ì¦ ê°€ëŠ¥
    
    db.close()

def delete_dropped_articles(db: DatabaseAdapter):
    """LLMì´ 'DROP'ìœ¼ë¡œ íŒì •í•œ ê¸°ì‚¬ë¥¼ DBì—ì„œ ì˜êµ¬ ì‚­ì œ"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ—‘ï¸ Cleanup: Deleting 'DROP' articles...")
    logger.info("="*60)
    
    try:
        cursor = db.connection.cursor()
        
        # 1. ì‚­ì œí•  ëŒ€ìƒ ì¡°íšŒ (processed_news)
        # DROPì¸ í•­ëª©ì˜ idì™€ ref_raw_idë¥¼ ì¡°íšŒ
        if DB_TYPE == "sqlite":
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
        else:
            query_select = "SELECT id, ref_raw_id FROM processed_news WHERE llm_decision = 'DROP'"
            
        cursor.execute(query_select)
        rows = cursor.fetchall()
        
        if not rows:
            logger.info("â„¹ï¸ No 'DROP' articles found to delete.")
            return

        p_ids = [row[0] for row in rows]
        r_ids = [row[1] for row in rows if row[1] is not None]
        
        logger.info(f"Found {len(p_ids)} articles marked as DROP.")
        
        # 2. processed_newsì—ì„œ ì‚­ì œ
        if DB_TYPE == "sqlite":
             placeholders_p = ",".join("?" * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
        else:
             placeholders_p = ",".join(["%s"] * len(p_ids))
             cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders_p})", tuple(p_ids))
             
        p_deleted = cursor.rowcount
        
        # 3. raw_newsì—ì„œ ì‚­ì œ
        if r_ids:
            if DB_TYPE == "sqlite":
                placeholders_r = ",".join("?" * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            else:
                placeholders_r = ",".join(["%s"] * len(r_ids))
                cursor.execute(f"DELETE FROM raw_news WHERE id IN ({placeholders_r})", tuple(r_ids))
            r_deleted = cursor.rowcount
        else:
            r_deleted = 0
            
        db.connection.commit()
        logger.info(f"âœ… Cleanup Complete: Deleted {p_deleted} processed_news rows and {r_deleted} raw_news rows.")
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete DROP articles: {e}")
        db.connection.rollback()


if __name__ == "__main__":
    main()
