#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 3: Keyword Filtering (run_p3.py)

ê¸°ëŠ¥:
1. processed_news í…Œì´ë¸”ì—ì„œ ê¸°ì‚¬ë¥¼ ì¡°íšŒ
2. ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ (DROP í‚¤ì›Œë“œ)ë¥¼ í¬í•¨í•œ ê¸°ì‚¬ í•„í„°ë§ (ì œê±°)
3. ì œê±°ëœ ê¸°ì‚¬ë¥¼ DBì—ì„œ ì‚­ì œ
4. ê²°ê³¼ í†µê³„ë¥¼ Google Sheetì— ì¶œë ¥

Usage:
    python run_p3.py
"""

import sys
import logging
import argparse
import re
from pathlib import Path
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from config.settings import (
    DB_TYPE, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,
    GOOGLE_SHEET_ID, LOG_LEVEL
)

# ë¡œê±° ì„¤ì •
logger = setup_logger(log_level=LOG_LEVEL)

# í•„í„°ë§ ê·œì¹™ ì •ì˜
DROP_KEYWORDS = [
    "ì˜ì…", "ì£¼ì‹ë‰´ìŠ¤", "ì´ë²¤íŠ¸", 
    "ì£¼ê±°ê¸‰ì—¬", "ì²­ë…„ì›”ì„¸", "ë‚œë°©ë¹„ì§€ì›", "ì¬ë‚œì§€ì›ê¸ˆ", "ë°”ìš°ì²˜", 
    "ë¬¸í™”ëˆ„ë¦¬ì¹´ë“œ", "ê·¼ë¡œì¥ë ¤ê¸ˆ", "ìë…€ì¥ë ¤ê¸ˆ", "ì§€ì›ê¸ˆ"
]

def check_drop_conditions(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì— DROP ì¡°ê±´ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    
    Returns:
        DROP ì‚¬ìœ  (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë“±) ë˜ëŠ” None
    """
    # 1. Regex: ì¸ì‚¬(?!ì´íŠ¸)
    if re.search(r'ì¸ì‚¬(?!ì´íŠ¸)', text):
        return "Rule: Noise Keyword (ì¸ì‚¬)"

    # 2. Literal: [í‘œ]
    if "[í‘œ]" in text:
        return "Rule: Noise Keyword ([í‘œ])"
    
    # 3. Keywords
    for kw in DROP_KEYWORDS:
        if re.search(r'\b' + re.escape(kw) + r'\b', text):
            return f"Rule: Drop Keyword '{kw}'"
            
    return None

def export_to_gsheet(stats: Dict[str, Any], sheet_id: str):
    """ê²°ê³¼ë¥¼ Google Sheetsì— ì¶œë ¥"""
    if not sheet_id:
        logger.warning("âš ï¸  GOOGLE_SHEET_ID not configured. Skipping export.")
        return
    
    tab_name = "3.keyword_filter"
    
    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        rows = []
        
        # 1. Statistics Header
        rows.append(["=== Phase 3 Keyword Filtering Statistics ===", ""])
        rows.append(["Total Input Articles", stats["total_input"]])
        rows.append(["Total Kept", stats["total_kept"]])
        rows.append(["Total Dropped", stats["total_dropped"]])
        if stats["total_input"] > 0:
            drop_rate = (stats["total_dropped"] / stats["total_input"] * 100)
        else:
            drop_rate = 0
        rows.append(["Drop Rate", f"{drop_rate:.1f}%"])
        rows.append(["", ""])
        
        # 2. Dropped Articles Detail
        if stats.get("dropped_examples"):
            rows.append(["=== DROPPED ARTICLES DETAILS ===", "", "", ""])
            rows.append(["ID", "Source", "Title", "Reason"])
            
            for item in stats["dropped_examples"]:
                rows.append([
                    item["id"],
                    item["source"],
                    item["title"][:100],
                    item["reason"]
                ])
        
        # Export rows
        if rows:
            adapter.worksheet.insert_rows(rows, 1)
        
        logger.info(f"âœ… ê²°ê³¼ë¥¼ Google Sheets '{tab_name}' íƒ­ì— ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"âŒ Google Sheets ì¶œë ¥ ì‹¤íŒ¨: {e}", exc_info=True)

def main():
    parser = argparse.ArgumentParser(description="Phase 3: Keyword Filtering")
    parser.add_argument("--no-export", action="store_true", help="Google Sheet ì¶œë ¥ ê±´ë„ˆë›°ê¸°")
    args = parser.parse_args()

    logger.info("\n" + "="*80)
    logger.info("ğŸš€ Phase 3 Start: Keyword Filtering (Removing Noise)")
    logger.info("="*80)
    
    # 1. DB ì—°ê²°
    try:
        db_adapter = DatabaseAdapter(
            db_type=DB_TYPE,
            host=DB_HOST if DB_TYPE != "sqlite" else None,
            port=DB_PORT if DB_TYPE != "sqlite" else None,
            database=DB_NAME,
            user=DB_USER if DB_TYPE != "sqlite" else None,
            password=DB_PASSWORD if DB_TYPE != "sqlite" else None
        )
        db_adapter.connect()
    except Exception as e:
        logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # 2. processed_news ê°€ì ¸ì˜¤ê¸°
    try:
        cursor = db_adapter.connection.cursor()
        cursor.execute("""
            SELECT id, source_normalized, ref_raw_id
            FROM processed_news
        """)
        processed_articles = cursor.fetchall() # id, source, ref_id
        
        logger.info(f"ğŸ“¥ Loaded {len(processed_articles)} articles from processed_news")
        
        dropped_ids = []
        dropped_examples = []
        kept_count = 0
        
        # raw_newsì—ì„œ title, snippet ì •ë³´ ì¡°ì¸ì„ ìœ„í•´ ë³„ë„ ì¡°íšŒ í˜¹ì€ ì¡°ì¸ ì¿¼ë¦¬ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ê°œë³„ ì¡°íšŒë³´ë‹¤ëŠ”, í•œë²ˆì— ì¡°ì¸í•´ì„œ ê°€ì ¸ì˜¤ëŠ”ê²Œ íš¨ìœ¨ì ì„.
        # ì¿¼ë¦¬ ìˆ˜ì •
        cursor.execute("""
            SELECT p.id, p.source_normalized, r.title, r.snippet
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
        """)
        
        articles_with_content = cursor.fetchall()
        
        for row in articles_with_content:
            p_id, source, title, snippet = row
            text = f"{title} {snippet}"
            
            drop_reason = check_drop_conditions(text)
            
            if drop_reason:
                dropped_ids.append(p_id)
                dropped_examples.append({
                    "id": p_id,
                    "source": source,
                    "title": title,
                    "reason": drop_reason
                })
                logger.debug(f"ğŸš« [DROP] {title[:30]}... ({drop_reason})")
            else:
                kept_count += 1
        
        # 3. DBì—ì„œ ì‚­ì œ ìˆ˜í–‰
        if dropped_ids:
            logger.info(f"ğŸ—‘ï¸  Deleting {len(dropped_ids)} dropped articles from processed_news...")
            
            # SQLite ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ ì‚­ì œ (ì˜ˆ: 900ê°œì”©)
            chunk_size = 900
            for i in range(0, len(dropped_ids), chunk_size):
                chunk = dropped_ids[i:i + chunk_size]
                placeholders = ', '.join(['?'] * len(chunk))
                cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders})", chunk)
            
            db_adapter.connection.commit()
            logger.info("âœ… Deletion complete.")
        else:
            logger.info("âœ¨ No articles matched DROP criteria.")
        
        # 4. ê²°ê³¼ í†µê³„
        stats = {
            "total_input": len(articles_with_content),
            "total_kept": kept_count,
            "total_dropped": len(dropped_ids),
            "dropped_examples": dropped_examples
        }
        
        # 5. ê²°ê³¼ ì¶œë ¥
        if not args.no_export:
            export_to_gsheet(stats, GOOGLE_SHEET_ID)
            
        logger.info("\n" + "="*80)
        logger.info("âœ… Phase 3 Filtering ì™„ë£Œ")
        logger.info("="*80)
        logger.info(f"  Input: {stats['total_input']}")
        logger.info(f"  Kept: {stats['total_kept']}")
        logger.info(f"  Dropped: {stats['total_dropped']}")
        
    except Exception as e:
        logger.error(f"âŒ Phase 3 Processing Error: {e}", exc_info=True)
    finally:
        db_adapter.close()

if __name__ == "__main__":
    main()
