#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 3: Keyword Filtering (run_p3.py)

Í∏∞Îä•:
1. processed_news ÌÖåÏù¥Î∏îÏóêÏÑú Í∏∞ÏÇ¨Î•º Ï°∞Ìöå
2. Î∂àÌïÑÏöîÌïú ÌÇ§ÏõåÎìú (DROP ÌÇ§ÏõåÎìú)Î•º Ìè¨Ìï®Ìïú Í∏∞ÏÇ¨ ÌïÑÌÑ∞ÎßÅ (Ï†úÍ±∞)
3. Ï†úÍ±∞Îêú Í∏∞ÏÇ¨Î•º DBÏóêÏÑú ÏÇ≠Ï†ú
4. Í≤∞Í≥º ÌÜµÍ≥ÑÎ•º Google SheetÏóê Ï∂úÎ†•

Usage:
    python run_p3.py
"""

import sys
import logging
import argparse
import re
from pathlib import Path
from typing import List, Dict, Any

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∂îÍ∞Ä
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from config.settings import (
    DB_TYPE, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD,
    GOOGLE_SHEET_ID, LOG_LEVEL
)

# Î°úÍ±∞ ÏÑ§Ï†ï
logger = setup_logger(log_level=LOG_LEVEL)

# ÌïÑÌÑ∞ÎßÅ Í∑úÏπô Ï†ïÏùò
DROP_KEYWORDS = [
    "ÏòÅÏûÖ", "Ï£ºÏãùÎâ¥Ïä§", "Ïù¥Î≤§Ìä∏", 
    "Ï£ºÍ±∞Í∏âÏó¨", "Ï≤≠ÎÖÑÏõîÏÑ∏", "ÎÇúÎ∞©ÎπÑÏßÄÏõê", "Ïû¨ÎÇúÏßÄÏõêÍ∏à", "Î∞îÏö∞Ï≤ò", 
    "Î¨∏ÌôîÎàÑÎ¶¨Ïπ¥Îìú", "Í∑ºÎ°úÏû•Î†§Í∏à", "ÏûêÎÖÄÏû•Î†§Í∏à", "ÏßÄÏõêÍ∏à",
    "Ïù∏ÎèÑ", "Î≤†Ìä∏ÎÇ®", "Î∏åÎùºÏßà", "India", "Vietnam", "Brazil"
]

# Ï†úÎ™© Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ Ìå®ÌÑ¥ (Regex) - ÏÇ≠Ï†ú ÎåÄÏÉÅ
# [Îã®ÎèÖ], [Ï¢ÖÌï©] Îì±ÏùÄ Ï†úÏô∏ (Ïú†ÏßÄ)
TITLE_DROP_PATTERNS = [
    r"^\[(Ìè¨ÌÜ†|ÏÜçÎ≥¥|Í∞úÏû•|ÎßàÍ∞ê|1Î≥¥|2Î≥¥|3Î≥¥|ÏÉÅÎ≥¥|Î∂ÄÍ≥†|Ïù∏ÏÇ¨|ÎèôÏ†ï|ÌñâÏÇ¨|ÏïåÎ¶º|Î™®Ïßë)\]"
]

def check_drop_conditions(text: str) -> str:
    """
    ÌÖçÏä§Ìä∏Ïóê DROP Ï°∞Í±¥Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏
    
    Returns:
        DROP ÏÇ¨Ïú† (Îß§Ïπ≠Îêú ÌÇ§ÏõåÎìú Îì±) ÎòêÎäî None
    """
    # 0. Title Prefix Patterns (Regex)
    for pattern in TITLE_DROP_PATTERNS:
        # textÎäî "Title Snippet" ÌòïÌÉúÏù¥ÎØÄÎ°ú ÏãúÏûë Î∂ÄÎ∂Ñ Îß§Ïπ≠ ÌôïÏù∏
        if re.search(pattern, text):
            return f"Rule: Title Prefix Drop"

    # 1. Regex: Ïù∏ÏÇ¨(?!Ïù¥Ìä∏)
    if re.search(r'Ïù∏ÏÇ¨(?!Ïù¥Ìä∏)', text):
        return "Rule: Noise Keyword (Ïù∏ÏÇ¨)"

    # 2. Literal: [Ìëú]
    if "[Ìëú]" in text:
        return "Rule: Noise Keyword ([Ìëú])"
    
    # 3. Keywords
    for kw in DROP_KEYWORDS:
        if re.search(r'\b' + re.escape(kw) + r'\b', text):
            return f"Rule: Drop Keyword '{kw}'"
            
    return None

def export_to_gsheet(stats: Dict[str, Any], sheet_id: str):
    """Í≤∞Í≥ºÎ•º Google SheetsÏóê Ï∂úÎ†•"""
    if not sheet_id:
        logger.warning("‚ö†Ô∏è  GOOGLE_SHEET_ID not configured. Skipping export.")
        return
    
    tab_name = "3.keyword_filter"
    
    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        rows = []
        
        # 1. Statistics Header
        rows.append(["=== Phase 3 Keyword Filtering Statistics ===", ""])
        rows.append(["Total Input Articles", stats["total_input"]])
        rows.append(["Total Kept", stats["total_kept"]])
        rows.append(["Total Dropped", stats["total_dropped"]])
        if stats["total_input"] > 0:
            drop_rate = (stats["total_dropped"] / stats["total_input"] * 100)
        else:
            drop_rate = 0
        rows.append(["Drop Rate", f"{drop_rate:.1f}%"])
        rows.append(["", ""])
        
        # 2. Dropped Articles Detail
        if stats.get("dropped_examples"):
            rows.append(["=== DROPPED ARTICLES DETAILS ===", "", "", ""])
            rows.append(["ID", "Source", "Title", "Reason"])
            
            for item in stats["dropped_examples"]:
                rows.append([
                    item["id"],
                    item["source"],
                    item["title"][:100],
                    item["reason"]
                ])
        
        # Export rows
        if rows:
            adapter.worksheet.insert_rows(rows, 1)
        
        logger.info(f"‚úÖ Í≤∞Í≥ºÎ•º Google Sheets '{tab_name}' ÌÉ≠Ïóê Ï∂úÎ†•ÌñàÏäµÎãàÎã§.")
        
    except Exception as e:
        logger.error(f"‚ùå Google Sheets Ï∂úÎ†• Ïã§Ìå®: {e}", exc_info=True)

def main():
    parser = argparse.ArgumentParser(description="Phase 3: Keyword Filtering")
    parser.add_argument("--no-export", action="store_true", help="Google Sheet Ï∂úÎ†• Í±¥ÎÑàÎõ∞Í∏∞")
    args = parser.parse_args()

    logger.info("\n" + "="*80)
    logger.info("üöÄ Phase 3 Start: Keyword Filtering (Removing Noise)")
    logger.info("="*80)
    
    # 1. DB Ïó∞Í≤∞
    try:
        db_adapter = DatabaseAdapter(
            db_type=DB_TYPE,
            host=DB_HOST if DB_TYPE != "sqlite" else None,
            port=DB_PORT if DB_TYPE != "sqlite" else None,
            database=DB_NAME,
            user=DB_USER if DB_TYPE != "sqlite" else None,
            password=DB_PASSWORD if DB_TYPE != "sqlite" else None
        )
        db_adapter.connect()
    except Exception as e:
        logger.error(f"‚ùå DB Ïó∞Í≤∞ Ïã§Ìå®: {e}")
        return

    # 2. processed_news Í∞ÄÏ†∏Ïò§Í∏∞
    try:
        cursor = db_adapter.connection.cursor()
        cursor.execute("""
            SELECT id, source_normalized, ref_raw_id
            FROM processed_news
        """)
        processed_articles = cursor.fetchall() # id, source, ref_id
        
        logger.info(f"üì• Loaded {len(processed_articles)} articles from processed_news")
        
        dropped_ids = []
        dropped_examples = []
        kept_count = 0
        
        # raw_newsÏóêÏÑú title, snippet Ï†ïÎ≥¥ Ï°∞Ïù∏ÏùÑ ÏúÑÌï¥ Î≥ÑÎèÑ Ï°∞Ìöå ÌòπÏùÄ Ï°∞Ïù∏ ÏøºÎ¶¨ ÏÇ¨Ïö©
        # Ïó¨Í∏∞ÏÑúÎäî Îã®ÏàúÌôîÎ•º ÏúÑÌï¥ Í∞úÎ≥Ñ Ï°∞ÌöåÎ≥¥Îã§Îäî, ÌïúÎ≤àÏóê Ï°∞Ïù∏Ìï¥ÏÑú Í∞ÄÏ†∏Ïò§ÎäîÍ≤å Ìö®Ïú®Ï†ÅÏûÑ.
        # ÏøºÎ¶¨ ÏàòÏ†ï
        cursor.execute("""
            SELECT p.id, p.source_normalized, r.title, r.snippet
            FROM processed_news p
            JOIN raw_news r ON p.ref_raw_id = r.id
        """)
        
        articles_with_content = cursor.fetchall()
        
        for row in articles_with_content:
            p_id, source, title, snippet = row
            text = f"{title} {snippet}"
            
            drop_reason = check_drop_conditions(text)
            
            if drop_reason:
                dropped_ids.append(p_id)
                dropped_examples.append({
                    "id": p_id,
                    "source": source,
                    "title": title,
                    "reason": drop_reason
                })
                logger.debug(f"üö´ [DROP] {title[:30]}... ({drop_reason})")
            else:
                kept_count += 1
        
        # 3. DBÏóêÏÑú ÏÇ≠Ï†ú ÏàòÌñâ
        if dropped_ids:
            logger.info(f"üóëÔ∏è  Deleting {len(dropped_ids)} dropped articles from processed_news...")
            
            # SQLite Ï†úÌïúÏùÑ Í≥†Î†§ÌïòÏó¨ Ï≤≠ÌÅ¨ Îã®ÏúÑ ÏÇ≠Ï†ú (Ïòà: 900Í∞úÏî©)
            chunk_size = 900
            for i in range(0, len(dropped_ids), chunk_size):
                chunk = dropped_ids[i:i + chunk_size]
                placeholders = ', '.join(['?'] * len(chunk))
                cursor.execute(f"DELETE FROM processed_news WHERE id IN ({placeholders})", chunk)
            
            db_adapter.connection.commit()
            logger.info("‚úÖ Deletion complete.")
        else:
            logger.info("‚ú® No articles matched DROP criteria.")
        
        # 4. Í≤∞Í≥º ÌÜµÍ≥Ñ
        stats = {
            "total_input": len(articles_with_content),
            "total_kept": kept_count,
            "total_dropped": len(dropped_ids),
            "dropped_examples": dropped_examples
        }
        
        # Stats Collection
        try:
            from src.utils.stats_collector import StatsCollector
            sc = StatsCollector()
            sc.set_stat("keyword_filtered", stats["total_kept"]) # Or total_dropped? Summary script expects 'keyword filtered' count? 
            # run_summary.py uses 'keyword_filtered' key.
            # Usually meant 'how many remain after filter' or 'how many filtered OUT'?
            # Actually, P3 is "Keyword Filtering". The summary column usually tracks "Survivor Count" at each stage.
            # Col 1: Total Collected
            # Col 2: Dedup Removed (This is removed count)
            # Col 3: Keyword Filtered (This could be meaningful as survivors or removed)
            # Let's check run_summary.py logic or standard pattern.
            # Let's log SURVIVORS for clarity or REMOVED?
            # run_summary just prints the value.
            # Let's store 'total_kept' as 'keyword_filtered_kept' ? Or store 'dropped'?
            # Let's stick to storing 'total_dropped' as 'keyword_filtered_dropped' if the column header implies action.
            # Run Summary Header: "Keyword Filtered" -> ambiguous.
            # Let's assumed it means "Articles Filtered Out". 
            sc.set_stat("keyword_filtered", stats["total_dropped"])
        except Exception as e:
            logger.error(f"Stats collection failed: {e}")
        
        # 5. Í≤∞Í≥º Ï∂úÎ†• (ÏÉùÎûµ)
        # if not args.no_export:
        #     export_to_gsheet(stats, GOOGLE_SHEET_ID)
        pass
            
        logger.info("\n" + "="*80)
        logger.info("‚úÖ Phase 3 Filtering ÏôÑÎ£å")
        logger.info("="*80)
        logger.info(f"  Input: {stats['total_input']}")
        logger.info(f"  Kept: {stats['total_kept']}")
        logger.info(f"  Dropped: {stats['total_dropped']}")
        
    except Exception as e:
        logger.error(f"‚ùå Phase 3 Processing Error: {e}", exc_info=True)
    finally:
        db_adapter.close()

if __name__ == "__main__":
    main()
