#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 5: Event Clustering (run_p5.py)

ê¸°ëŠ¥:
1. Phase 4ì—ì„œ 'KEEP'ìœ¼ë¡œ ë¶„ë¥˜ëœ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜´ (news.db)
2. ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
3. Step 1: 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ (Reason ê¸°ë°˜) -> ì‹œíŠ¸ 5.topics_step1 ë° DB ì €ì¥
   (Step 2 Refinement ê³¼ì • ì‚­ì œë¨)

Usage:
    python run_p5.py
"""

import sys
import json
import logging
import argparse
import sqlite3
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime, timedelta
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from storage.db_adapter import DatabaseAdapter
from src.exporters.gsheet import GSheetAdapter
from src.utils.logger import setup_logger
from src.utils.timezone_utils import format_kst_date
from config.settings import (
    DB_TYPE, DB_NAME, LOG_LEVEL, OPENAI_API_KEY, GOOGLE_API_KEY, GEMINI_MODEL, GOOGLE_SHEET_ID, BASE_DIR
)
from config.prompts.topic_clustering import get_topic_clustering_prompt
from src.processors.similarity_deduplicator import SimilarityDeduplicator

# Google Generative AI check
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = setup_logger(log_level=LOG_LEVEL)

TOPICS_DB_PATH = BASE_DIR / "data" / "topics.db"


class TopicsDB:
    """topics.db ì „ìš© ê°„ë‹¨í•œ ì–´ëŒ‘í„°"""
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.connection = None
        self.cursor = None

    def connect(self):
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.connection = sqlite3.connect(self.db_path)
        self.connection.row_factory = sqlite3.Row
        self.cursor = self.connection.cursor()
        self._create_table()

    def _create_table(self):
        query = """
            CREATE TABLE IF NOT EXISTS topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                topic_title TEXT,
                news_ids TEXT, -- JSON List of IDs
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """
        self.cursor.execute(query)
        self.connection.commit()

    def insert_topic(self, category: str, topic_title: str, news_ids: List[int]):
        query = "INSERT INTO topics (category, topic_title, news_ids) VALUES (?, ?, ?)"
        self.cursor.execute(query, (category, topic_title, json.dumps(news_ids)))
        self.connection.commit()
    
    def clear_topics(self):
        self.cursor.execute("DELETE FROM topics")
        self.connection.commit()

    def get_all_topics(self):
        self.cursor.execute("SELECT * FROM topics ORDER BY id DESC")
        return self.cursor.fetchall()

    def close(self):
        if self.connection:
            self.connection.close()

    def reset_db(self):
        """Drop and recreate table"""
        self.cursor.execute("DROP TABLE IF EXISTS topics")
        self.connection.commit()
        self._create_table()
        logger.warning("âš ï¸  topics.db has been reset (table dropped and recreated).")



def get_keep_articles(db: DatabaseAdapter, hours: int = 24) -> List[Dict[str, Any]]:
    """KEEP ê¸°ì‚¬ ì¡°íšŒ (ìµœê·¼ Nì‹œê°„ ë‚´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë§Œ)"""
    try:
        cursor = db.connection.cursor()
        
        # ì‹œê°„ í•„í„° ê³„ì‚° (UTC ê¸°ì¤€)
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        # Publisher ì •ë³´ë„ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°
        if DB_TYPE == "sqlite":
             query = """
                SELECT p.id, r.title, p.llm_category, p.llm_reason, COALESCE(r.publisher, r.source) as pub
                FROM processed_news p
                JOIN raw_news r ON p.ref_raw_id = r.id
                WHERE p.llm_decision = 'KEEP'
                  AND r.collected_at >= ?
                ORDER BY p.id DESC
            """
             cursor.execute(query, (cutoff_time,))
        else:
             query = """
                SELECT p.id, r.title, p.llm_category, p.llm_reason, COALESCE(r.publisher, r.source) as pub
                FROM processed_news p
                JOIN raw_news r ON p.ref_raw_id = r.id
                WHERE p.llm_decision = 'KEEP'
                  AND r.collected_at >= %s
                ORDER BY p.id DESC
            """
             cursor.execute(query, (cutoff_time,))
            
        rows = cursor.fetchall()
        
        articles = []
        for row in rows:
            articles.append({
                "id": row[0],
                "title": row[1],
                "category": row[2],
                "reason": row[3],
                "publisher": row[4],
                "source": row[4] # For deduplicator source tier check
            })
        return articles
    except Exception as e:
        logger.error(f"âŒ Failed to fetch KEEP articles: {e}")
        return []



def clean_llm_json_output(conversation: str) -> str:
    """LLM ì¶œë ¥ì—ì„œ Markdown ì½”ë“œ ë¸”ë¡ ë° ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°"""
    import re
    cleaned_content = conversation.strip()
    
    # 1. Markdown code block extraction
    match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", cleaned_content)
    if match:
        return match.group(1).strip()
    
    # 2. If no code block, try to find the first '[' (since we expect a list)
    #    or '{' if we expected a dict (but here we mainly expect list).
    #    Let's just return stripped content if no code block found, 
    #    but remove common prefixes if needed? 
    #    Actually, simple replacement of backticks might be safer as fallback
    if "```" in cleaned_content:
        cleaned_content = cleaned_content.replace("```json", "").replace("```", "").strip()
        
    return cleaned_content


def robust_json_load(content: str) -> Any:
    """JSON íŒŒì‹± ì‹œë„ (ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬)"""
    try:
        parsed = json.loads(content)
        return parsed
    except json.JSONDecodeError as e:
        logger.error(f"JSON Parsing Error: {e}")
        logger.error(f"Failed Content Preview: {content[:500]}...")  # Log first 500 chars
        return []


def normalize_clusters(parsed: Any) -> List[Dict[str, Any]]:
    """LLM ì¶œë ¥ì„ í‘œì¤€ í¬ë§·(List[Dict])ìœ¼ë¡œ ë³€í™˜"""
    clusters = []
    if isinstance(parsed, dict):
        # {"topics": [...]} í˜•íƒœ ì‹œë„
        found_list = False
        for key, value in parsed.items():
            if isinstance(value, list):
                clusters = value
                found_list = True
                break
        
        if not found_list:
            clusters = [parsed]
            
    elif isinstance(parsed, list):
        clusters = parsed
        
    valid_clusters = []
    for c in clusters:
        if isinstance(c, dict):
            # Key normalization: "topic" or "topic_title" -> "topic"
            if "topic_title" in c and "topic" not in c:
                c["topic"] = c["topic_title"]
            valid_clusters.append(c)
            
    return valid_clusters


def cluster_step1(model: Any, category: str, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Step 1: 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ (Reason ê¸°ë°˜) - Gemini Version"""
    if not articles:
        return []
    
    logger.info(f"ğŸ¤– [Step 1] Clustering {len(articles)} articles in '{category}'...")

    system_prompt = get_topic_clustering_prompt()
    
    input_data = [
        {"i": a["id"], "r": a["reason"]} # id -> i, reason -> r
        for a in articles
    ]
    # Minified JSON
    user_content = json.dumps(input_data, ensure_ascii=False, separators=(',', ':'))
    
    # Update prompt slightly to inform LLM of key change (implicitly) 
    # Actually, Gemini is smart enough to infer i=id, r=reason from context if we just say "list of articles".
    # But for safety, we can prepend a tiny legend or let it infer. 
    # Given the prompt says "Each item has an id and a reason", mapping i/r is trivial for Gemini 1.5/2.0.
    full_prompt = f"{system_prompt}\n\nCategory: {category}\n\nArticles (i=id, r=reason):\n{user_content}"

    try:
        response = model.generate_content(full_prompt)
        content = clean_llm_json_output(response.text)
        parsed = robust_json_load(content)
        return normalize_clusters(parsed)

    except Exception as e:
        logger.error(f"âŒ Step 1 Failed for '{category}': {e}")
        return []


def translate_titles(model: Any, titles: List[str]) -> List[str]:
    """
    Translate foreign article titles to Korean using Gemini.
    Returns list of translated titles (or empty string if already Korean).
    """
    if not titles:
        return []
    
    # Filter out empty titles
    non_empty_titles = [(i, title) for i, title in enumerate(titles) if title]
    
    if not non_empty_titles:
        return ["" for _ in titles]
    
    logger.info(f"ğŸŒ Translating {len(non_empty_titles)} article titles to Korean...")
    
    # Create a prompt for batch translation
    # Optimization: id -> i, title -> t, Minified JSON
    titles_data = [{"i": i, "t": title} for i, title in non_empty_titles]
    titles_json = json.dumps(titles_data, ensure_ascii=False, separators=(',', ':'))
    
    prompt = f"""You are a professional translator. Translate the following article titles to Korean.

Rules:
1. If a title is already in Korean, return an empty string "" for that title
2. If a title is in a foreign language (English, etc.), translate it to natural Korean
3. Keep proper nouns (company names, people names, places) in their original form
4. Return ONLY a valid JSON array in this exact format:
[
  {{"i": 0, "t": "ë²ˆì—­ëœ ì œëª© ë˜ëŠ” ë¹ˆ ë¬¸ìì—´"}},
  {{"i": 1, "t": "ë²ˆì—­ëœ ì œëª© ë˜ëŠ” ë¹ˆ ë¬¸ìì—´"}}
]

Titles to translate (i=id, t=title):
{titles_json}

Return ONLY the JSON array, no other text."""
    
    try:
        response = model.generate_content(prompt)
        content = clean_llm_json_output(response.text)
        parsed = robust_json_load(content)
        
        # Create result array with same length as input
        translations = ["" for _ in titles]
        
        if isinstance(parsed, list):
            for item in parsed:
                # Optimized keys: i=id, t=translation
                if isinstance(item, dict):
                    idx = item.get("i")
                    trans_text = item.get("t")
                    
                    if idx is not None:
                        # Map back to original indices
                        for orig_idx, orig_title in non_empty_titles:
                            if orig_idx == idx:
                                translations[orig_idx] = trans_text or ""
                                break
        
        return translations
    
    except Exception as e:
        logger.error(f"âŒ Translation Failed: {e}")
        return ["" for _ in titles]


def init_sheet(sheet_id: str, tab_name: str):
    """ì‹œíŠ¸ ì´ˆê¸°í™” (Clear & Header)"""
    if not sheet_id:
        return

    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        adapter.worksheet.clear()
        
        # í—¤ë”
        headers = [
            ["Category", "Topic Title", "News Count", "Reason", "Publisher", "Title", "Title (Korean)", "URL"]
        ]
        adapter.worksheet.insert_rows(headers, 1)
        logger.info(f"âœ… Initialized sheet '{tab_name}'")
    except Exception as e:
        logger.error(f"âŒ Sheet Init Failed ({tab_name}): {e}")


def append_topics_to_sheet(sheet_id: str, tab_name: str, topic_list: List[Dict[str, Any]], news_db: DatabaseAdapter, model: Any = None):
    """ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ê¸€ ì‹œíŠ¸ì— ì¶”ê°€ (Append)"""
    if not sheet_id or not topic_list:
        return

    try:
        adapter = GSheetAdapter(sheet_id=sheet_id, worksheet_name=tab_name)
        adapter.connect()
        
        # Sort topic_list by news count DESC before appending
        # (Within the category, larger topics first)
        topic_list.sort(key=lambda x: -len(x.get("news_ids", [])))

        sheet_rows = []
        
        for topic in topic_list:
            category = topic.get("category", "")
            title = topic.get("topic", "")
            news_ids = topic.get("news_ids", [])
            
            if not news_ids:
                continue
                
            # ë‰´ìŠ¤ ì •ë³´ ì¡°íšŒ
            placeholders = ",".join(["?"] * len(news_ids)) if DB_TYPE == "sqlite" else ",".join(["%s"] * len(news_ids))
            
            cursor = news_db.connection.cursor()
            
            if DB_TYPE == "sqlite":
                q = f"""
                    SELECT r.title, p.llm_reason, COALESCE(r.publisher, r.source) as pub, r.url
                    FROM processed_news p
                    JOIN raw_news r ON p.ref_raw_id = r.id
                    WHERE p.id IN ({placeholders})
                """
                cursor.execute(q, tuple(news_ids))
            else:
                q = f"""
                    SELECT r.title, p.llm_reason, COALESCE(r.publisher, r.source) as pub, r.url
                    FROM processed_news p
                    JOIN raw_news r ON p.ref_raw_id = r.id
                    WHERE p.id IN ({placeholders})
                """
                cursor.execute(q, tuple(news_ids))
            
            rows = cursor.fetchall()
            
            titles_list = []
            urls_list = []
            for r in rows:
                title_text = r[0]
                url = r[3]
                titles_list.append(title_text)
                urls_list.append(url if url else "")

            reasons_list = [r[1] or "" for r in rows]
            pubs_list = [r[2] or "" for r in rows]
            
            reasons_str = "\n".join(reasons_list)
            pubs_str = "\n".join(pubs_list)
            titles_str = "\n".join(titles_list)
            urls_str = "\n".join(urls_list)
            
            
            # Use Google Sheets GOOGLETRANSLATE formula instead of LLM
            # Formula will auto-translate foreign titles to Korean
            # Note: We need to track the current row number in the sheet
            # Since we're appending, we need to calculate the row offset
            
            # Get current row count to calculate formula row numbers
            current_row = len(adapter.worksheet.get_all_values()) + 1  # +1 for next row
            
            # Generate GOOGLETRANSLATE formulas for each title line
            title_lines = titles_str.split('\n')
            formula_parts = []
            for i, _ in enumerate(title_lines):
                # Each line in the multi-line cell needs its own translation
                # We'll use a single formula that handles the entire multi-line cell
                pass
            
            # Simple approach: Use one formula for the entire cell
            # GOOGLETRANSLATE will handle multi-line text
            titles_kr_formula = f'=IF(ISBLANK(F{current_row}), "", GOOGLETRANSLATE(F{current_row}, "auto", "ko"))'
            
            sheet_rows.append([category, title, len(news_ids), reasons_str, pubs_str, titles_str, titles_kr_formula, urls_str])

        if sheet_rows:
            adapter.worksheet.append_rows(sheet_rows)
            logger.info(f"âœ… Appended {len(topic_list)} topics to sheet '{tab_name}'")

    except Exception as e:
        logger.error(f"âŒ Sheet Append Failed ({tab_name}): {e}")


def main():
    parser = argparse.ArgumentParser(description="Phase 5: Event Clustering")
    parser.add_argument("--no-export", action="store_true", help="Sheet ì¶œë ¥ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--reset-db", action="store_true", help="DB ì´ˆê¸°í™” (í…Œì´ë¸” ì‚­ì œ í›„ ì¬ìƒì„±)")
    args = parser.parse_args()

    # 2. DB ì—°ê²° (Topics DB)
    topics_db = TopicsDB(TOPICS_DB_PATH)
    topics_db.connect()

    if args.reset_db:
        topics_db.reset_db()
        topics_db.close()
        return

    if not GENAI_AVAILABLE:
        logger.error("âŒ google-generativeai package is not installed.")
        return
        
    if not GOOGLE_API_KEY:
        logger.error("âŒ GOOGLE_API_KEY is missing.")
        return

    logger.info("="*80)
    logger.info(f"ğŸš€ Phase 5: Event Clustering Start (Model: {GEMINI_MODEL})")
    logger.info("="*80)

    # 1. DB ì—°ê²° (News DB)
    news_db = DatabaseAdapter(db_type=DB_TYPE, database=DB_NAME)
    news_db.connect()

    # DB ì´ˆê¸°í™” (ì´ë¯¸ ìœ„ì—ì„œ ì—°ê²°ë¨, ì¼ë°˜ ì‹¤í–‰ì‹œëŠ” clear_topics í˜¸ì¶œ)
    # topics_db is already connected above
    topics_db.clear_topics()

    # 3. KEEP ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    articles = get_keep_articles(news_db)
    logger.info(f"ğŸ“¥ Found {len(articles)} KEEP articles.")
    
    # [NEW] Similarity Deduplication
    try:
        deduplicator = SimilarityDeduplicator(similarity_threshold=0.5)
        dedup_result = deduplicator.run(articles)
        articles = dedup_result["articles"]
    except Exception as e:
        logger.error(f"âš ï¸ Deduplication failed, proceeding with original list: {e}")
    
    # 4. ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
    grouped = defaultdict(list)
    for a in articles:
        cat = a["category"] or "Uncategorized"
        grouped[cat].append(a)

    # Initialize Gemini
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL)
    
    # ê²°ê³¼ë¥¼ ëª¨ì„ ë¦¬ìŠ¤íŠ¸
    all_results = []
    
    # 5. ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜í–‰
    # Sort categories to ensure deterministic order (e.g. alphabetical)
    sorted_categories = sorted(grouped.keys())

    # 4. Sheet Init (Clear & Headers) - BEFORE Loop
    tab_name = format_kst_date("%y%m%d")
    # tab_name = "5.Topics"  
    if not args.no_export:
        init_sheet(GOOGLE_SHEET_ID, tab_name)
    
    # 5. ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜í–‰
    for category in sorted_categories:
        items = grouped[category]
        if len(items) == 0:
            continue
            
        logger.info(f"Processing Category: {category} ({len(items)} articles)")
        
        # --- Step 1: Clustering ---
        step1_clusters = cluster_step1(model, category, items)
        
        # Add category info to results
        for c in step1_clusters:
            c["category"] = category
            
        all_results.extend(step1_clusters)
        
        # DB ì €ì¥ (ë°”ë¡œ ì €ì¥)
        for cluster in step1_clusters:
            topic_title = cluster.get("topic")
            news_ids = cluster.get("news_ids", [])
            if topic_title and news_ids:
                topics_db.insert_topic(category, topic_title, news_ids)
        
        # Sheet Append (Incremental)
        if not args.no_export:
            # Append only this category's clusters
            append_topics_to_sheet(
                GOOGLE_SHEET_ID, 
                tab_name,
                step1_clusters, 
                news_db,
                model  # Pass model for title translation
            )

    logger.info(f"âœ… Completed. Generated {len(all_results)} topics.")


    news_db.close()
    topics_db.close()


if __name__ == "__main__":
    main()
